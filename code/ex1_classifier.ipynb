{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6vooAYGoIxX"
   },
   "source": [
    "<h4 style=\"direction:rtl;\" align=\"center\">بسم الله الرحمن الرحیم</h4>\n",
    "<br>\n",
    "<h4 style=\"direction:rtl;\" align=\"center\">دانشگاه صنعتی شریف - بهار ۱۴۰۲</h4>\n",
    "<h3 style=\"direction:rtl;\" align=\"center\">درس سامانه‌های یادگیری ماشین</h3>\n",
    "<hr>\n",
    "<div><h2 style=\"direction:rtl;\" align=\"center\">تمرین اول - آزمایش اول</h2><div>\n",
    "<hr>\n",
    "<h4 style=\"direction:rtl;\" align=\"center\">دانشجو: محمدحسین موثقی‌نیا</h4>\n",
    "<h4 style=\"direction:rtl;\" align=\"center\">شماره دانشجویی: ۴۰۰۲۰۰۹۱۹</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3IsWPB351bI"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wM5K1HUiEN4d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqBz3mjBHZaS"
   },
   "source": [
    "### preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "1iOh9UJTGO14"
   },
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/MLSD/hw1/data/feature_eng/ex1_tSNE_data.csv /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 661
    },
    "id": "5BJCn7P1zH9W",
    "outputId": "6019491a-7b4e-4531-d9a1-fb1a5e60ce18"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-be9c524b-5aeb-46d4-954b-144fea6f19c4\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>electronic-devices</td>\n",
       "      <td>computers</td>\n",
       "      <td>laptop-notebook-macbook</td>\n",
       "      <td>0.753740</td>\n",
       "      <td>1.423870</td>\n",
       "      <td>-1.721890</td>\n",
       "      <td>-2.578469</td>\n",
       "      <td>0.844065</td>\n",
       "      <td>1.404051</td>\n",
       "      <td>-2.280355</td>\n",
       "      <td>1.289248</td>\n",
       "      <td>-1.571081</td>\n",
       "      <td>-0.371342</td>\n",
       "      <td>-0.234636</td>\n",
       "      <td>-1.295279</td>\n",
       "      <td>-0.252478</td>\n",
       "      <td>0.589191</td>\n",
       "      <td>-0.417869</td>\n",
       "      <td>-2.322152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>electronic-devices</td>\n",
       "      <td>computers</td>\n",
       "      <td>laptop-notebook-macbook</td>\n",
       "      <td>1.261034</td>\n",
       "      <td>-0.135896</td>\n",
       "      <td>0.819901</td>\n",
       "      <td>0.080539</td>\n",
       "      <td>2.470107</td>\n",
       "      <td>2.114458</td>\n",
       "      <td>-0.782585</td>\n",
       "      <td>2.748756</td>\n",
       "      <td>-1.403517</td>\n",
       "      <td>0.276262</td>\n",
       "      <td>0.843485</td>\n",
       "      <td>-1.578507</td>\n",
       "      <td>-0.399329</td>\n",
       "      <td>-0.989666</td>\n",
       "      <td>-0.652857</td>\n",
       "      <td>-3.591844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>electronic-devices</td>\n",
       "      <td>computers</td>\n",
       "      <td>laptop-notebook-macbook</td>\n",
       "      <td>-0.160877</td>\n",
       "      <td>-0.312552</td>\n",
       "      <td>-0.378412</td>\n",
       "      <td>1.606118</td>\n",
       "      <td>0.564957</td>\n",
       "      <td>0.488024</td>\n",
       "      <td>0.442798</td>\n",
       "      <td>-0.430075</td>\n",
       "      <td>1.823981</td>\n",
       "      <td>0.570655</td>\n",
       "      <td>-1.216096</td>\n",
       "      <td>-1.204440</td>\n",
       "      <td>-0.099236</td>\n",
       "      <td>-0.258720</td>\n",
       "      <td>1.303967</td>\n",
       "      <td>0.340199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>electronic-devices</td>\n",
       "      <td>computers</td>\n",
       "      <td>laptop-notebook-macbook</td>\n",
       "      <td>1.016462</td>\n",
       "      <td>0.008083</td>\n",
       "      <td>0.480749</td>\n",
       "      <td>-0.152555</td>\n",
       "      <td>1.985879</td>\n",
       "      <td>1.938341</td>\n",
       "      <td>-0.489907</td>\n",
       "      <td>2.011956</td>\n",
       "      <td>-1.507131</td>\n",
       "      <td>-0.453977</td>\n",
       "      <td>1.242481</td>\n",
       "      <td>-1.332945</td>\n",
       "      <td>0.034277</td>\n",
       "      <td>-0.918605</td>\n",
       "      <td>-0.714148</td>\n",
       "      <td>-3.167405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>electronic-devices</td>\n",
       "      <td>computers</td>\n",
       "      <td>laptop-notebook-macbook</td>\n",
       "      <td>-1.181108</td>\n",
       "      <td>1.468957</td>\n",
       "      <td>-0.076279</td>\n",
       "      <td>-3.581999</td>\n",
       "      <td>-1.778966</td>\n",
       "      <td>-1.067209</td>\n",
       "      <td>-0.898841</td>\n",
       "      <td>0.221498</td>\n",
       "      <td>-1.955891</td>\n",
       "      <td>0.499015</td>\n",
       "      <td>0.634064</td>\n",
       "      <td>0.524263</td>\n",
       "      <td>-0.015860</td>\n",
       "      <td>0.755366</td>\n",
       "      <td>-0.298483</td>\n",
       "      <td>1.393452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>vehicles</td>\n",
       "      <td>auto</td>\n",
       "      <td>car</td>\n",
       "      <td>2.492277</td>\n",
       "      <td>0.467670</td>\n",
       "      <td>-2.349543</td>\n",
       "      <td>-0.445706</td>\n",
       "      <td>-2.816162</td>\n",
       "      <td>-0.502231</td>\n",
       "      <td>0.186543</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>1.205143</td>\n",
       "      <td>-0.040757</td>\n",
       "      <td>-0.839839</td>\n",
       "      <td>0.711829</td>\n",
       "      <td>-1.682932</td>\n",
       "      <td>-1.085025</td>\n",
       "      <td>-0.777322</td>\n",
       "      <td>-0.036514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>vehicles</td>\n",
       "      <td>auto</td>\n",
       "      <td>car</td>\n",
       "      <td>1.692873</td>\n",
       "      <td>-1.684629</td>\n",
       "      <td>2.967271</td>\n",
       "      <td>-0.872879</td>\n",
       "      <td>-3.734276</td>\n",
       "      <td>-0.810425</td>\n",
       "      <td>-1.256816</td>\n",
       "      <td>0.861264</td>\n",
       "      <td>2.016335</td>\n",
       "      <td>1.495117</td>\n",
       "      <td>-0.520023</td>\n",
       "      <td>0.764754</td>\n",
       "      <td>-1.261731</td>\n",
       "      <td>-0.748036</td>\n",
       "      <td>-0.682958</td>\n",
       "      <td>1.405245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>vehicles</td>\n",
       "      <td>auto</td>\n",
       "      <td>car</td>\n",
       "      <td>1.760449</td>\n",
       "      <td>-1.104733</td>\n",
       "      <td>1.631837</td>\n",
       "      <td>-0.489250</td>\n",
       "      <td>-3.183470</td>\n",
       "      <td>1.654977</td>\n",
       "      <td>-1.513304</td>\n",
       "      <td>-0.046626</td>\n",
       "      <td>0.847650</td>\n",
       "      <td>0.734327</td>\n",
       "      <td>-0.060871</td>\n",
       "      <td>0.164739</td>\n",
       "      <td>0.026395</td>\n",
       "      <td>0.640612</td>\n",
       "      <td>0.136390</td>\n",
       "      <td>0.095577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>vehicles</td>\n",
       "      <td>auto</td>\n",
       "      <td>car</td>\n",
       "      <td>3.056003</td>\n",
       "      <td>-0.458365</td>\n",
       "      <td>-0.597599</td>\n",
       "      <td>-0.975780</td>\n",
       "      <td>-3.691450</td>\n",
       "      <td>-1.134242</td>\n",
       "      <td>-0.032662</td>\n",
       "      <td>1.753517</td>\n",
       "      <td>1.030465</td>\n",
       "      <td>1.212188</td>\n",
       "      <td>-0.852422</td>\n",
       "      <td>1.002553</td>\n",
       "      <td>-0.377149</td>\n",
       "      <td>-1.149037</td>\n",
       "      <td>-1.115173</td>\n",
       "      <td>0.850334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>vehicles</td>\n",
       "      <td>auto</td>\n",
       "      <td>car</td>\n",
       "      <td>3.037320</td>\n",
       "      <td>-0.245059</td>\n",
       "      <td>-0.782734</td>\n",
       "      <td>-0.873090</td>\n",
       "      <td>-3.630301</td>\n",
       "      <td>-0.957559</td>\n",
       "      <td>-0.231566</td>\n",
       "      <td>1.598884</td>\n",
       "      <td>0.969659</td>\n",
       "      <td>1.106295</td>\n",
       "      <td>-0.815587</td>\n",
       "      <td>1.122735</td>\n",
       "      <td>-0.614745</td>\n",
       "      <td>-1.076181</td>\n",
       "      <td>-1.032468</td>\n",
       "      <td>0.558671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1428 rows × 19 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be9c524b-5aeb-46d4-954b-144fea6f19c4')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-be9c524b-5aeb-46d4-954b-144fea6f19c4 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-be9c524b-5aeb-46d4-954b-144fea6f19c4');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                    cat1       cat2                     cat3         0  \\\n",
       "0     electronic-devices  computers  laptop-notebook-macbook  0.753740   \n",
       "1     electronic-devices  computers  laptop-notebook-macbook  1.261034   \n",
       "2     electronic-devices  computers  laptop-notebook-macbook -0.160877   \n",
       "3     electronic-devices  computers  laptop-notebook-macbook  1.016462   \n",
       "4     electronic-devices  computers  laptop-notebook-macbook -1.181108   \n",
       "...                  ...        ...                      ...       ...   \n",
       "1423            vehicles       auto                      car  2.492277   \n",
       "1424            vehicles       auto                      car  1.692873   \n",
       "1425            vehicles       auto                      car  1.760449   \n",
       "1426            vehicles       auto                      car  3.056003   \n",
       "1427            vehicles       auto                      car  3.037320   \n",
       "\n",
       "             1         2         3         4         5         6         7  \\\n",
       "0     1.423870 -1.721890 -2.578469  0.844065  1.404051 -2.280355  1.289248   \n",
       "1    -0.135896  0.819901  0.080539  2.470107  2.114458 -0.782585  2.748756   \n",
       "2    -0.312552 -0.378412  1.606118  0.564957  0.488024  0.442798 -0.430075   \n",
       "3     0.008083  0.480749 -0.152555  1.985879  1.938341 -0.489907  2.011956   \n",
       "4     1.468957 -0.076279 -3.581999 -1.778966 -1.067209 -0.898841  0.221498   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1423  0.467670 -2.349543 -0.445706 -2.816162 -0.502231  0.186543  0.805556   \n",
       "1424 -1.684629  2.967271 -0.872879 -3.734276 -0.810425 -1.256816  0.861264   \n",
       "1425 -1.104733  1.631837 -0.489250 -3.183470  1.654977 -1.513304 -0.046626   \n",
       "1426 -0.458365 -0.597599 -0.975780 -3.691450 -1.134242 -0.032662  1.753517   \n",
       "1427 -0.245059 -0.782734 -0.873090 -3.630301 -0.957559 -0.231566  1.598884   \n",
       "\n",
       "             8         9        10        11        12        13        14  \\\n",
       "0    -1.571081 -0.371342 -0.234636 -1.295279 -0.252478  0.589191 -0.417869   \n",
       "1    -1.403517  0.276262  0.843485 -1.578507 -0.399329 -0.989666 -0.652857   \n",
       "2     1.823981  0.570655 -1.216096 -1.204440 -0.099236 -0.258720  1.303967   \n",
       "3    -1.507131 -0.453977  1.242481 -1.332945  0.034277 -0.918605 -0.714148   \n",
       "4    -1.955891  0.499015  0.634064  0.524263 -0.015860  0.755366 -0.298483   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1423  1.205143 -0.040757 -0.839839  0.711829 -1.682932 -1.085025 -0.777322   \n",
       "1424  2.016335  1.495117 -0.520023  0.764754 -1.261731 -0.748036 -0.682958   \n",
       "1425  0.847650  0.734327 -0.060871  0.164739  0.026395  0.640612  0.136390   \n",
       "1426  1.030465  1.212188 -0.852422  1.002553 -0.377149 -1.149037 -1.115173   \n",
       "1427  0.969659  1.106295 -0.815587  1.122735 -0.614745 -1.076181 -1.032468   \n",
       "\n",
       "            15  \n",
       "0    -2.322152  \n",
       "1    -3.591844  \n",
       "2     0.340199  \n",
       "3    -3.167405  \n",
       "4     1.393452  \n",
       "...        ...  \n",
       "1423 -0.036514  \n",
       "1424  1.405245  \n",
       "1425  0.095577  \n",
       "1426  0.850334  \n",
       "1427  0.558671  \n",
       "\n",
       "[1428 rows x 19 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/content/ex1_tSNE_data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "feALEfaWoldE",
    "outputId": "7d36db54-94ae-4ae9-84fa-14e13fb75e45"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-cab08f11-40aa-43ba-aac4-96b0a89b7c46\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.753740</td>\n",
       "      <td>1.423870</td>\n",
       "      <td>-1.721890</td>\n",
       "      <td>-2.578469</td>\n",
       "      <td>0.844065</td>\n",
       "      <td>1.404051</td>\n",
       "      <td>-2.280355</td>\n",
       "      <td>1.289248</td>\n",
       "      <td>-1.571081</td>\n",
       "      <td>-0.371342</td>\n",
       "      <td>-0.234636</td>\n",
       "      <td>-1.295279</td>\n",
       "      <td>-0.252478</td>\n",
       "      <td>0.589191</td>\n",
       "      <td>-0.417869</td>\n",
       "      <td>-2.322152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.261034</td>\n",
       "      <td>-0.135896</td>\n",
       "      <td>0.819901</td>\n",
       "      <td>0.080539</td>\n",
       "      <td>2.470107</td>\n",
       "      <td>2.114458</td>\n",
       "      <td>-0.782585</td>\n",
       "      <td>2.748756</td>\n",
       "      <td>-1.403517</td>\n",
       "      <td>0.276262</td>\n",
       "      <td>0.843485</td>\n",
       "      <td>-1.578507</td>\n",
       "      <td>-0.399329</td>\n",
       "      <td>-0.989666</td>\n",
       "      <td>-0.652857</td>\n",
       "      <td>-3.591844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.160877</td>\n",
       "      <td>-0.312552</td>\n",
       "      <td>-0.378412</td>\n",
       "      <td>1.606118</td>\n",
       "      <td>0.564957</td>\n",
       "      <td>0.488024</td>\n",
       "      <td>0.442798</td>\n",
       "      <td>-0.430075</td>\n",
       "      <td>1.823981</td>\n",
       "      <td>0.570655</td>\n",
       "      <td>-1.216096</td>\n",
       "      <td>-1.204440</td>\n",
       "      <td>-0.099236</td>\n",
       "      <td>-0.258720</td>\n",
       "      <td>1.303967</td>\n",
       "      <td>0.340199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.016462</td>\n",
       "      <td>0.008083</td>\n",
       "      <td>0.480749</td>\n",
       "      <td>-0.152555</td>\n",
       "      <td>1.985879</td>\n",
       "      <td>1.938341</td>\n",
       "      <td>-0.489907</td>\n",
       "      <td>2.011956</td>\n",
       "      <td>-1.507131</td>\n",
       "      <td>-0.453977</td>\n",
       "      <td>1.242481</td>\n",
       "      <td>-1.332945</td>\n",
       "      <td>0.034277</td>\n",
       "      <td>-0.918605</td>\n",
       "      <td>-0.714148</td>\n",
       "      <td>-3.167405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.181108</td>\n",
       "      <td>1.468957</td>\n",
       "      <td>-0.076279</td>\n",
       "      <td>-3.581999</td>\n",
       "      <td>-1.778966</td>\n",
       "      <td>-1.067209</td>\n",
       "      <td>-0.898841</td>\n",
       "      <td>0.221498</td>\n",
       "      <td>-1.955891</td>\n",
       "      <td>0.499015</td>\n",
       "      <td>0.634064</td>\n",
       "      <td>0.524263</td>\n",
       "      <td>-0.015860</td>\n",
       "      <td>0.755366</td>\n",
       "      <td>-0.298483</td>\n",
       "      <td>1.393452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>2.492277</td>\n",
       "      <td>0.467670</td>\n",
       "      <td>-2.349543</td>\n",
       "      <td>-0.445706</td>\n",
       "      <td>-2.816162</td>\n",
       "      <td>-0.502231</td>\n",
       "      <td>0.186543</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>1.205143</td>\n",
       "      <td>-0.040757</td>\n",
       "      <td>-0.839839</td>\n",
       "      <td>0.711829</td>\n",
       "      <td>-1.682932</td>\n",
       "      <td>-1.085025</td>\n",
       "      <td>-0.777322</td>\n",
       "      <td>-0.036514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>1.692873</td>\n",
       "      <td>-1.684629</td>\n",
       "      <td>2.967271</td>\n",
       "      <td>-0.872879</td>\n",
       "      <td>-3.734276</td>\n",
       "      <td>-0.810425</td>\n",
       "      <td>-1.256816</td>\n",
       "      <td>0.861264</td>\n",
       "      <td>2.016335</td>\n",
       "      <td>1.495117</td>\n",
       "      <td>-0.520023</td>\n",
       "      <td>0.764754</td>\n",
       "      <td>-1.261731</td>\n",
       "      <td>-0.748036</td>\n",
       "      <td>-0.682958</td>\n",
       "      <td>1.405245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>1.760449</td>\n",
       "      <td>-1.104733</td>\n",
       "      <td>1.631837</td>\n",
       "      <td>-0.489250</td>\n",
       "      <td>-3.183470</td>\n",
       "      <td>1.654977</td>\n",
       "      <td>-1.513304</td>\n",
       "      <td>-0.046626</td>\n",
       "      <td>0.847650</td>\n",
       "      <td>0.734327</td>\n",
       "      <td>-0.060871</td>\n",
       "      <td>0.164739</td>\n",
       "      <td>0.026395</td>\n",
       "      <td>0.640612</td>\n",
       "      <td>0.136390</td>\n",
       "      <td>0.095577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>3.056003</td>\n",
       "      <td>-0.458365</td>\n",
       "      <td>-0.597599</td>\n",
       "      <td>-0.975780</td>\n",
       "      <td>-3.691450</td>\n",
       "      <td>-1.134242</td>\n",
       "      <td>-0.032662</td>\n",
       "      <td>1.753517</td>\n",
       "      <td>1.030465</td>\n",
       "      <td>1.212188</td>\n",
       "      <td>-0.852422</td>\n",
       "      <td>1.002553</td>\n",
       "      <td>-0.377149</td>\n",
       "      <td>-1.149037</td>\n",
       "      <td>-1.115173</td>\n",
       "      <td>0.850334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>3.037320</td>\n",
       "      <td>-0.245059</td>\n",
       "      <td>-0.782734</td>\n",
       "      <td>-0.873090</td>\n",
       "      <td>-3.630301</td>\n",
       "      <td>-0.957559</td>\n",
       "      <td>-0.231566</td>\n",
       "      <td>1.598884</td>\n",
       "      <td>0.969659</td>\n",
       "      <td>1.106295</td>\n",
       "      <td>-0.815587</td>\n",
       "      <td>1.122735</td>\n",
       "      <td>-0.614745</td>\n",
       "      <td>-1.076181</td>\n",
       "      <td>-1.032468</td>\n",
       "      <td>0.558671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1428 rows × 16 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cab08f11-40aa-43ba-aac4-96b0a89b7c46')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-cab08f11-40aa-43ba-aac4-96b0a89b7c46 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-cab08f11-40aa-43ba-aac4-96b0a89b7c46');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.753740  1.423870 -1.721890 -2.578469  0.844065  1.404051 -2.280355   \n",
       "1     1.261034 -0.135896  0.819901  0.080539  2.470107  2.114458 -0.782585   \n",
       "2    -0.160877 -0.312552 -0.378412  1.606118  0.564957  0.488024  0.442798   \n",
       "3     1.016462  0.008083  0.480749 -0.152555  1.985879  1.938341 -0.489907   \n",
       "4    -1.181108  1.468957 -0.076279 -3.581999 -1.778966 -1.067209 -0.898841   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1423  2.492277  0.467670 -2.349543 -0.445706 -2.816162 -0.502231  0.186543   \n",
       "1424  1.692873 -1.684629  2.967271 -0.872879 -3.734276 -0.810425 -1.256816   \n",
       "1425  1.760449 -1.104733  1.631837 -0.489250 -3.183470  1.654977 -1.513304   \n",
       "1426  3.056003 -0.458365 -0.597599 -0.975780 -3.691450 -1.134242 -0.032662   \n",
       "1427  3.037320 -0.245059 -0.782734 -0.873090 -3.630301 -0.957559 -0.231566   \n",
       "\n",
       "             7         8         9        10        11        12        13  \\\n",
       "0     1.289248 -1.571081 -0.371342 -0.234636 -1.295279 -0.252478  0.589191   \n",
       "1     2.748756 -1.403517  0.276262  0.843485 -1.578507 -0.399329 -0.989666   \n",
       "2    -0.430075  1.823981  0.570655 -1.216096 -1.204440 -0.099236 -0.258720   \n",
       "3     2.011956 -1.507131 -0.453977  1.242481 -1.332945  0.034277 -0.918605   \n",
       "4     0.221498 -1.955891  0.499015  0.634064  0.524263 -0.015860  0.755366   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1423  0.805556  1.205143 -0.040757 -0.839839  0.711829 -1.682932 -1.085025   \n",
       "1424  0.861264  2.016335  1.495117 -0.520023  0.764754 -1.261731 -0.748036   \n",
       "1425 -0.046626  0.847650  0.734327 -0.060871  0.164739  0.026395  0.640612   \n",
       "1426  1.753517  1.030465  1.212188 -0.852422  1.002553 -0.377149 -1.149037   \n",
       "1427  1.598884  0.969659  1.106295 -0.815587  1.122735 -0.614745 -1.076181   \n",
       "\n",
       "            14        15  \n",
       "0    -0.417869 -2.322152  \n",
       "1    -0.652857 -3.591844  \n",
       "2     1.303967  0.340199  \n",
       "3    -0.714148 -3.167405  \n",
       "4    -0.298483  1.393452  \n",
       "...        ...       ...  \n",
       "1423 -0.777322 -0.036514  \n",
       "1424 -0.682958  1.405245  \n",
       "1425  0.136390  0.095577  \n",
       "1426 -1.115173  0.850334  \n",
       "1427 -1.032468  0.558671  \n",
       "\n",
       "[1428 rows x 16 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df = df[[f'{i}' for i in range(16)]]\n",
    "X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "OGr0Z__QHYqp",
    "outputId": "bd130660-338f-4813-8ad5-046811d801de"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-c4567a35-0e52-4f32-a24e-77672a9703a7\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1428 rows × 3 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c4567a35-0e52-4f32-a24e-77672a9703a7')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-c4567a35-0e52-4f32-a24e-77672a9703a7 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-c4567a35-0e52-4f32-a24e-77672a9703a7');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      cat1  cat2  cat3\n",
       "0        0     4     6\n",
       "1        0     4     6\n",
       "2        0     4     6\n",
       "3        0     4     6\n",
       "4        0     4     6\n",
       "...    ...   ...   ...\n",
       "1423     3     1     5\n",
       "1424     3     1     5\n",
       "1425     3     1     5\n",
       "1426     3     1     5\n",
       "1427     3     1     5\n",
       "\n",
       "[1428 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df = {}\n",
    "cat1_2_id = {t:i for i, t in enumerate(list(np.unique(df['cat1'])))}\n",
    "cat2_2_id = {t:i for i, t in enumerate(list(np.unique(df['cat2'])))}\n",
    "cat3_2_id = {t:i for i, t in enumerate(list(np.unique(df['cat3'])))}\n",
    "y_df['cat1'] = [cat1_2_id[t] for t in df['cat1']]\n",
    "y_df['cat2'] = [cat2_2_id[t] for t in df['cat2']]\n",
    "y_df['cat3'] = [cat3_2_id[t] for t in df['cat3']]\n",
    "y_df = pd.DataFrame(y_df)\n",
    "y_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wbmnINI4fp8"
   },
   "source": [
    "### Split train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "NSkBqtlOFqTA"
   },
   "outputs": [],
   "source": [
    "X = {'train': None, 'test': None, 'val': None}\n",
    "y = {'train': None, 'test': None, 'val': None}\n",
    "\n",
    "X_trainval, X['test'], y_trainval, y['test'] = train_test_split(X_df, y_df, \n",
    "                                                                test_size=0.2, \n",
    "                                                                stratify=y_df, \n",
    "                                                                shuffle=True, \n",
    "                                                                random_state=69)\n",
    "\n",
    "X['train'], X['val'], y['train'], y['val'] = train_test_split(X_trainval, \n",
    "                                                              y_trainval,\n",
    "                                                              test_size=0.1, \n",
    "                                                              stratify=y_trainval, \n",
    "                                                              shuffle=True, \n",
    "                                                              random_state=21)\n",
    "\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X['train'] = scaler.fit_transform(X['train'])\n",
    "X['val'] = scaler.transform(X['val'])\n",
    "X['test'] = scaler.transform(X['test'])\n",
    "X['train'], y['train'] = np.array(X['train']).astype('float32'), np.array(y['train'])\n",
    "X['val'], y['val'] = np.array(X['val']).astype('float32'), np.array(y['val'])\n",
    "X['test'], y['test'] = np.array(X['test']).astype('float32'), np.array(y['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCDeExvo4jgf"
   },
   "source": [
    "### Create dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "LjHgCrOrw95l"
   },
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, x_train, y_train):\n",
    "        self.x = x_train\n",
    "        self.y = y_train\n",
    "        self.len = self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self,index):      \n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCMeNuLt4rRW"
   },
   "source": [
    "Set hyper-parameter:\n",
    "\n",
    "- batch-size: 256\n",
    "- weighted-loss: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "LDShwxjoxwkw"
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "weighted_loss = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataloaders = {}\n",
    "dataset_sizes = {}\n",
    "\n",
    "for phase in ['train', 'val', 'test']:\n",
    "    data_set = Data(X[phase], y[phase])\n",
    "    dataset_sizes[phase] = data_set.len\n",
    "    dataloaders[phase] = DataLoader(dataset=data_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLnlRXA6417Z"
   },
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "6MR_BI5dUuUq"
   },
   "outputs": [],
   "source": [
    "class MyClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, out1_dim, out2_dim, out3_dim):\n",
    "        super(MyClassifier, self).__init__()\n",
    "        self.in_layer = nn.Linear(in_dim, 64)\n",
    "        self.bn0 = nn.BatchNorm1d(64)\n",
    "        self.hidden1_layer = nn.Linear(64, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.hidden2_layer = nn.Linear(64, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.out1_layer = nn.Linear(32, out1_dim)\n",
    "        self.out2_layer = nn.Linear(32+out1_dim, out2_dim)\n",
    "        self.out3_layer = nn.Linear(32+out1_dim+out2_dim, out3_dim)\n",
    "        self.soft_max = nn.Softmax(dim=1)\n",
    "    \n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.in_layer(x)\n",
    "        x = self.bn0(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.hidden1_layer(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.hidden2_layer(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        out1 = self.out1_layer(x)\n",
    "\n",
    "        out2 = torch.cat([x, out1], dim=1)\n",
    "        out2 = self.out2_layer(out2)\n",
    "\n",
    "        out3 = torch.cat([x, out1, out2], dim=1)\n",
    "        out3 = self.out3_layer(out3)\n",
    "\n",
    "        out1 = self.soft_max(out1)\n",
    "        out2 = self.soft_max(out2)\n",
    "        out3 = self.soft_max(out3)\n",
    "\n",
    "        return out1, out2, out3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpiByh2844LG"
   },
   "source": [
    "### Define Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "pN9awMXQEIUL"
   },
   "outputs": [],
   "source": [
    "criterion = [None, None, None]\n",
    "\n",
    "if weighted_loss:\n",
    "    class_weights = [None]*3\n",
    "    for i in range(3):\n",
    "        class_weights[i] = compute_class_weight(class_weight = \"balanced\", \n",
    "                                                classes = np.unique(np.array(y['train'])[:,i]), \n",
    "                                                y = np.array(y['train'])[:,i])\n",
    "        class_weights[i] = torch.tensor(class_weights[i], dtype=torch.float)\n",
    "        criterion[i] = nn.CrossEntropyLoss(weight=class_weights[i])\n",
    "else:\n",
    "    criterion[0] = nn.CrossEntropyLoss()\n",
    "    criterion[1] = nn.CrossEntropyLoss()\n",
    "    criterion[2] = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHRQ7j1546cL"
   },
   "source": [
    "### Load model and set optimizer to Adam\n",
    "\n",
    "Set hyper-parameter:\n",
    "\n",
    "- a, b, c: 1, 2, 5 (Cost function coefficients for different categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "SYwmTyOAx18i"
   },
   "outputs": [],
   "source": [
    "model = MyClassifier(in_dim=X['train'].shape[1], \n",
    "                     out1_dim=len(np.unique(y_df['cat1'])), \n",
    "                     out2_dim=len(np.unique(y_df['cat2'])), \n",
    "                     out3_dim=len(np.unique(y_df['cat3']))\n",
    "                     )\n",
    "\n",
    "learning_rate = 2e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "a = 1\n",
    "b = 2\n",
    "c = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_VYAtyt5NjY"
   },
   "source": [
    "### Define train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "l3rTX9hqSsLL"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = [100, 100, 100]\n",
    "    loss_history = {'train': [], 'val': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = [0,0,0]\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = [None, None, None]\n",
    "                    for i in range(3):\n",
    "                        loss[i] = criterion[i](outputs[i], labels[:,i])\n",
    "                        running_loss[i] += loss[i].item()\n",
    "                    total_loss = a*loss[0] + b*loss[1] + c*loss[2]\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        total_loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "            for i in range(3):\n",
    "                running_loss[i] = running_loss[i]/dataset_sizes[phase]\n",
    "                print(f'{phase} Loss[cat{i}]: {running_loss[i]:.6f}')\n",
    "            \n",
    "            loss_history[phase].append(running_loss)\n",
    "\n",
    "            # deep copy the model\n",
    "            diff = 1e-5\n",
    "            if phase == 'val' and (running_loss[0]+diff < best_loss[0] or running_loss[1]+diff < best_loss[1] or running_loss[2]+diff < best_loss[2]):\n",
    "                best_loss = running_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                print('** Model Saved **')\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YedK_FyY5RHf"
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xNaSoHCAnaa8",
    "outputId": "cb4f51a5-1c47-40b7-a875-88e36c09bc66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014951\n",
      "val Loss[cat2]: 0.018537\n",
      "\n",
      "Epoch 3446/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004503\n",
      "train Loss[cat1]: 0.007021\n",
      "train Loss[cat2]: 0.008610\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014952\n",
      "val Loss[cat2]: 0.018539\n",
      "\n",
      "Epoch 3447/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004503\n",
      "train Loss[cat1]: 0.007021\n",
      "train Loss[cat2]: 0.008610\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014952\n",
      "val Loss[cat2]: 0.018538\n",
      "\n",
      "Epoch 3448/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004503\n",
      "train Loss[cat1]: 0.007021\n",
      "train Loss[cat2]: 0.008610\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014951\n",
      "val Loss[cat2]: 0.018537\n",
      "\n",
      "Epoch 3449/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004503\n",
      "train Loss[cat1]: 0.007021\n",
      "train Loss[cat2]: 0.008609\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014952\n",
      "val Loss[cat2]: 0.018538\n",
      "\n",
      "Epoch 3450/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004502\n",
      "train Loss[cat1]: 0.007020\n",
      "train Loss[cat2]: 0.008609\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014953\n",
      "val Loss[cat2]: 0.018538\n",
      "\n",
      "Epoch 3451/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004502\n",
      "train Loss[cat1]: 0.007020\n",
      "train Loss[cat2]: 0.008609\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014952\n",
      "val Loss[cat2]: 0.018538\n",
      "\n",
      "Epoch 3452/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004502\n",
      "train Loss[cat1]: 0.007020\n",
      "train Loss[cat2]: 0.008608\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014952\n",
      "val Loss[cat2]: 0.018538\n",
      "\n",
      "Epoch 3453/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004502\n",
      "train Loss[cat1]: 0.007019\n",
      "train Loss[cat2]: 0.008608\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014952\n",
      "val Loss[cat2]: 0.018538\n",
      "\n",
      "Epoch 3454/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004502\n",
      "train Loss[cat1]: 0.007019\n",
      "train Loss[cat2]: 0.008608\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014953\n",
      "val Loss[cat2]: 0.018538\n",
      "\n",
      "Epoch 3455/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004502\n",
      "train Loss[cat1]: 0.007019\n",
      "train Loss[cat2]: 0.008607\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014954\n",
      "val Loss[cat2]: 0.018538\n",
      "\n",
      "Epoch 3456/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004501\n",
      "train Loss[cat1]: 0.007018\n",
      "train Loss[cat2]: 0.008607\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014953\n",
      "val Loss[cat2]: 0.018538\n",
      "\n",
      "Epoch 3457/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004501\n",
      "train Loss[cat1]: 0.007018\n",
      "train Loss[cat2]: 0.008606\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014953\n",
      "val Loss[cat2]: 0.018538\n",
      "\n",
      "Epoch 3458/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004501\n",
      "train Loss[cat1]: 0.007018\n",
      "train Loss[cat2]: 0.008606\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014953\n",
      "val Loss[cat2]: 0.018538\n",
      "\n",
      "Epoch 3459/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004500\n",
      "train Loss[cat1]: 0.007018\n",
      "train Loss[cat2]: 0.008606\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014955\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3460/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004500\n",
      "train Loss[cat1]: 0.007017\n",
      "train Loss[cat2]: 0.008605\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014955\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3461/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004500\n",
      "train Loss[cat1]: 0.007017\n",
      "train Loss[cat2]: 0.008605\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014954\n",
      "val Loss[cat2]: 0.018538\n",
      "\n",
      "Epoch 3462/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004500\n",
      "train Loss[cat1]: 0.007017\n",
      "train Loss[cat2]: 0.008604\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014954\n",
      "val Loss[cat2]: 0.018538\n",
      "\n",
      "Epoch 3463/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004500\n",
      "train Loss[cat1]: 0.007016\n",
      "train Loss[cat2]: 0.008604\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014955\n",
      "val Loss[cat2]: 0.018539\n",
      "\n",
      "Epoch 3464/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004500\n",
      "train Loss[cat1]: 0.007016\n",
      "train Loss[cat2]: 0.008604\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014954\n",
      "val Loss[cat2]: 0.018538\n",
      "\n",
      "Epoch 3465/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004499\n",
      "train Loss[cat1]: 0.007016\n",
      "train Loss[cat2]: 0.008603\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014955\n",
      "val Loss[cat2]: 0.018539\n",
      "\n",
      "Epoch 3466/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004499\n",
      "train Loss[cat1]: 0.007015\n",
      "train Loss[cat2]: 0.008603\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014955\n",
      "val Loss[cat2]: 0.018539\n",
      "\n",
      "Epoch 3467/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004499\n",
      "train Loss[cat1]: 0.007015\n",
      "train Loss[cat2]: 0.008603\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014955\n",
      "val Loss[cat2]: 0.018539\n",
      "\n",
      "Epoch 3468/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004499\n",
      "train Loss[cat1]: 0.007015\n",
      "train Loss[cat2]: 0.008602\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014955\n",
      "val Loss[cat2]: 0.018539\n",
      "\n",
      "Epoch 3469/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004498\n",
      "train Loss[cat1]: 0.007014\n",
      "train Loss[cat2]: 0.008602\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014957\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3470/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004498\n",
      "train Loss[cat1]: 0.007014\n",
      "train Loss[cat2]: 0.008601\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014956\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3471/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004498\n",
      "train Loss[cat1]: 0.007014\n",
      "train Loss[cat2]: 0.008601\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014955\n",
      "val Loss[cat2]: 0.018538\n",
      "\n",
      "Epoch 3472/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004498\n",
      "train Loss[cat1]: 0.007014\n",
      "train Loss[cat2]: 0.008601\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014956\n",
      "val Loss[cat2]: 0.018539\n",
      "\n",
      "Epoch 3473/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004497\n",
      "train Loss[cat1]: 0.007013\n",
      "train Loss[cat2]: 0.008600\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014956\n",
      "val Loss[cat2]: 0.018539\n",
      "\n",
      "Epoch 3474/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004497\n",
      "train Loss[cat1]: 0.007013\n",
      "train Loss[cat2]: 0.008600\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014955\n",
      "val Loss[cat2]: 0.018539\n",
      "\n",
      "Epoch 3475/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004497\n",
      "train Loss[cat1]: 0.007013\n",
      "train Loss[cat2]: 0.008599\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014954\n",
      "val Loss[cat2]: 0.018538\n",
      "\n",
      "Epoch 3476/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004497\n",
      "train Loss[cat1]: 0.007012\n",
      "train Loss[cat2]: 0.008599\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014956\n",
      "val Loss[cat2]: 0.018539\n",
      "\n",
      "Epoch 3477/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004497\n",
      "train Loss[cat1]: 0.007012\n",
      "train Loss[cat2]: 0.008599\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014957\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3478/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004496\n",
      "train Loss[cat1]: 0.007012\n",
      "train Loss[cat2]: 0.008598\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014956\n",
      "val Loss[cat2]: 0.018539\n",
      "\n",
      "Epoch 3479/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004496\n",
      "train Loss[cat1]: 0.007011\n",
      "train Loss[cat2]: 0.008598\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014956\n",
      "val Loss[cat2]: 0.018539\n",
      "\n",
      "Epoch 3480/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004496\n",
      "train Loss[cat1]: 0.007011\n",
      "train Loss[cat2]: 0.008598\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014957\n",
      "val Loss[cat2]: 0.018539\n",
      "\n",
      "Epoch 3481/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004496\n",
      "train Loss[cat1]: 0.007011\n",
      "train Loss[cat2]: 0.008597\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014959\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3482/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004496\n",
      "train Loss[cat1]: 0.007010\n",
      "train Loss[cat2]: 0.008597\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014958\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3483/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004495\n",
      "train Loss[cat1]: 0.007010\n",
      "train Loss[cat2]: 0.008597\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014958\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3484/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004495\n",
      "train Loss[cat1]: 0.007010\n",
      "train Loss[cat2]: 0.008596\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014959\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3485/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004495\n",
      "train Loss[cat1]: 0.007010\n",
      "train Loss[cat2]: 0.008596\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014958\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3486/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004495\n",
      "train Loss[cat1]: 0.007009\n",
      "train Loss[cat2]: 0.008595\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014958\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3487/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004495\n",
      "train Loss[cat1]: 0.007009\n",
      "train Loss[cat2]: 0.008595\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014959\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3488/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004495\n",
      "train Loss[cat1]: 0.007009\n",
      "train Loss[cat2]: 0.008595\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014959\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3489/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004494\n",
      "train Loss[cat1]: 0.007008\n",
      "train Loss[cat2]: 0.008594\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014958\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3490/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004494\n",
      "train Loss[cat1]: 0.007008\n",
      "train Loss[cat2]: 0.008594\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014958\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3491/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004494\n",
      "train Loss[cat1]: 0.007008\n",
      "train Loss[cat2]: 0.008593\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014959\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3492/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004494\n",
      "train Loss[cat1]: 0.007007\n",
      "train Loss[cat2]: 0.008593\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014960\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3493/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004494\n",
      "train Loss[cat1]: 0.007007\n",
      "train Loss[cat2]: 0.008593\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014958\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3494/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004493\n",
      "train Loss[cat1]: 0.007007\n",
      "train Loss[cat2]: 0.008592\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014958\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3495/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004493\n",
      "train Loss[cat1]: 0.007007\n",
      "train Loss[cat2]: 0.008592\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014959\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3496/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004493\n",
      "train Loss[cat1]: 0.007006\n",
      "train Loss[cat2]: 0.008592\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014959\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3497/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004493\n",
      "train Loss[cat1]: 0.007006\n",
      "train Loss[cat2]: 0.008591\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014959\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3498/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004492\n",
      "train Loss[cat1]: 0.007006\n",
      "train Loss[cat2]: 0.008591\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014959\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3499/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004492\n",
      "train Loss[cat1]: 0.007005\n",
      "train Loss[cat2]: 0.008590\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014959\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3500/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004492\n",
      "train Loss[cat1]: 0.007005\n",
      "train Loss[cat2]: 0.008590\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014959\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3501/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004492\n",
      "train Loss[cat1]: 0.007005\n",
      "train Loss[cat2]: 0.008590\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014959\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3502/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004492\n",
      "train Loss[cat1]: 0.007004\n",
      "train Loss[cat2]: 0.008589\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014959\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3503/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004491\n",
      "train Loss[cat1]: 0.007004\n",
      "train Loss[cat2]: 0.008589\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014959\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3504/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004491\n",
      "train Loss[cat1]: 0.007004\n",
      "train Loss[cat2]: 0.008589\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014959\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3505/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004491\n",
      "train Loss[cat1]: 0.007003\n",
      "train Loss[cat2]: 0.008588\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014959\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3506/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004491\n",
      "train Loss[cat1]: 0.007003\n",
      "train Loss[cat2]: 0.008588\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014960\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3507/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004491\n",
      "train Loss[cat1]: 0.007003\n",
      "train Loss[cat2]: 0.008587\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014961\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3508/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004490\n",
      "train Loss[cat1]: 0.007003\n",
      "train Loss[cat2]: 0.008587\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014960\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3509/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004490\n",
      "train Loss[cat1]: 0.007002\n",
      "train Loss[cat2]: 0.008587\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014960\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3510/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004490\n",
      "train Loss[cat1]: 0.007002\n",
      "train Loss[cat2]: 0.008586\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014961\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3511/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004490\n",
      "train Loss[cat1]: 0.007002\n",
      "train Loss[cat2]: 0.008586\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014960\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3512/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004490\n",
      "train Loss[cat1]: 0.007001\n",
      "train Loss[cat2]: 0.008585\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014960\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3513/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004489\n",
      "train Loss[cat1]: 0.007001\n",
      "train Loss[cat2]: 0.008585\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014960\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3514/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004489\n",
      "train Loss[cat1]: 0.007001\n",
      "train Loss[cat2]: 0.008585\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014961\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3515/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004489\n",
      "train Loss[cat1]: 0.007001\n",
      "train Loss[cat2]: 0.008584\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014961\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3516/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004489\n",
      "train Loss[cat1]: 0.007000\n",
      "train Loss[cat2]: 0.008584\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014961\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3517/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004489\n",
      "train Loss[cat1]: 0.007000\n",
      "train Loss[cat2]: 0.008584\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014961\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3518/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004488\n",
      "train Loss[cat1]: 0.007000\n",
      "train Loss[cat2]: 0.008583\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014962\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3519/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004488\n",
      "train Loss[cat1]: 0.006999\n",
      "train Loss[cat2]: 0.008583\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014962\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3520/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004488\n",
      "train Loss[cat1]: 0.006999\n",
      "train Loss[cat2]: 0.008582\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014963\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3521/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004488\n",
      "train Loss[cat1]: 0.006999\n",
      "train Loss[cat2]: 0.008582\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014965\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3522/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004488\n",
      "train Loss[cat1]: 0.006998\n",
      "train Loss[cat2]: 0.008582\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014963\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3523/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004487\n",
      "train Loss[cat1]: 0.006998\n",
      "train Loss[cat2]: 0.008581\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014963\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3524/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004487\n",
      "train Loss[cat1]: 0.006998\n",
      "train Loss[cat2]: 0.008581\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014965\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3525/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004487\n",
      "train Loss[cat1]: 0.006998\n",
      "train Loss[cat2]: 0.008581\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014964\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3526/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004487\n",
      "train Loss[cat1]: 0.006997\n",
      "train Loss[cat2]: 0.008580\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014964\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3527/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004487\n",
      "train Loss[cat1]: 0.006997\n",
      "train Loss[cat2]: 0.008580\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014965\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3528/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004486\n",
      "train Loss[cat1]: 0.006997\n",
      "train Loss[cat2]: 0.008579\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014963\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3529/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004486\n",
      "train Loss[cat1]: 0.006996\n",
      "train Loss[cat2]: 0.008579\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014964\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3530/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004486\n",
      "train Loss[cat1]: 0.006996\n",
      "train Loss[cat2]: 0.008579\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014964\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3531/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004486\n",
      "train Loss[cat1]: 0.006996\n",
      "train Loss[cat2]: 0.008578\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014965\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3532/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004486\n",
      "train Loss[cat1]: 0.006995\n",
      "train Loss[cat2]: 0.008578\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014966\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3533/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004485\n",
      "train Loss[cat1]: 0.006995\n",
      "train Loss[cat2]: 0.008578\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014964\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3534/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004485\n",
      "train Loss[cat1]: 0.006995\n",
      "train Loss[cat2]: 0.008577\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014965\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3535/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004485\n",
      "train Loss[cat1]: 0.006995\n",
      "train Loss[cat2]: 0.008577\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014965\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3536/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004485\n",
      "train Loss[cat1]: 0.006994\n",
      "train Loss[cat2]: 0.008576\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014964\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3537/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004485\n",
      "train Loss[cat1]: 0.006994\n",
      "train Loss[cat2]: 0.008576\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014966\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3538/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004484\n",
      "train Loss[cat1]: 0.006994\n",
      "train Loss[cat2]: 0.008576\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014967\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3539/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004484\n",
      "train Loss[cat1]: 0.006993\n",
      "train Loss[cat2]: 0.008575\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014966\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3540/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004484\n",
      "train Loss[cat1]: 0.006993\n",
      "train Loss[cat2]: 0.008575\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014966\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3541/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004484\n",
      "train Loss[cat1]: 0.006993\n",
      "train Loss[cat2]: 0.008574\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014966\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3542/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004484\n",
      "train Loss[cat1]: 0.006993\n",
      "train Loss[cat2]: 0.008574\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014966\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3543/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004483\n",
      "train Loss[cat1]: 0.006992\n",
      "train Loss[cat2]: 0.008574\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014967\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3544/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004483\n",
      "train Loss[cat1]: 0.006992\n",
      "train Loss[cat2]: 0.008573\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014967\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3545/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004483\n",
      "train Loss[cat1]: 0.006992\n",
      "train Loss[cat2]: 0.008573\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014967\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3546/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004483\n",
      "train Loss[cat1]: 0.006991\n",
      "train Loss[cat2]: 0.008573\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014967\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3547/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004482\n",
      "train Loss[cat1]: 0.006991\n",
      "train Loss[cat2]: 0.008572\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014967\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3548/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004482\n",
      "train Loss[cat1]: 0.006991\n",
      "train Loss[cat2]: 0.008572\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014967\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3549/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004482\n",
      "train Loss[cat1]: 0.006990\n",
      "train Loss[cat2]: 0.008571\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014967\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3550/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004482\n",
      "train Loss[cat1]: 0.006990\n",
      "train Loss[cat2]: 0.008571\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014967\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3551/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004482\n",
      "train Loss[cat1]: 0.006990\n",
      "train Loss[cat2]: 0.008571\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014968\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3552/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004481\n",
      "train Loss[cat1]: 0.006989\n",
      "train Loss[cat2]: 0.008570\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014968\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3553/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004481\n",
      "train Loss[cat1]: 0.006989\n",
      "train Loss[cat2]: 0.008570\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014969\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3554/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004481\n",
      "train Loss[cat1]: 0.006989\n",
      "train Loss[cat2]: 0.008570\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014968\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3555/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004481\n",
      "train Loss[cat1]: 0.006989\n",
      "train Loss[cat2]: 0.008569\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014968\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3556/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004481\n",
      "train Loss[cat1]: 0.006988\n",
      "train Loss[cat2]: 0.008569\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014968\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3557/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004481\n",
      "train Loss[cat1]: 0.006988\n",
      "train Loss[cat2]: 0.008569\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014968\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3558/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004480\n",
      "train Loss[cat1]: 0.006988\n",
      "train Loss[cat2]: 0.008568\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014968\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3559/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004480\n",
      "train Loss[cat1]: 0.006988\n",
      "train Loss[cat2]: 0.008568\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014968\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3560/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004480\n",
      "train Loss[cat1]: 0.006987\n",
      "train Loss[cat2]: 0.008567\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014967\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3561/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004480\n",
      "train Loss[cat1]: 0.006987\n",
      "train Loss[cat2]: 0.008567\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014968\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3562/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004479\n",
      "train Loss[cat1]: 0.006986\n",
      "train Loss[cat2]: 0.008567\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014971\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3563/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004479\n",
      "train Loss[cat1]: 0.006986\n",
      "train Loss[cat2]: 0.008566\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014970\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3564/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004479\n",
      "train Loss[cat1]: 0.006986\n",
      "train Loss[cat2]: 0.008566\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014969\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3565/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004479\n",
      "train Loss[cat1]: 0.006986\n",
      "train Loss[cat2]: 0.008566\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014969\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3566/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004479\n",
      "train Loss[cat1]: 0.006985\n",
      "train Loss[cat2]: 0.008565\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014971\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3567/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004479\n",
      "train Loss[cat1]: 0.006985\n",
      "train Loss[cat2]: 0.008565\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014968\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3568/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004478\n",
      "train Loss[cat1]: 0.006985\n",
      "train Loss[cat2]: 0.008564\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014969\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3569/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004478\n",
      "train Loss[cat1]: 0.006984\n",
      "train Loss[cat2]: 0.008564\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014970\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3570/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004478\n",
      "train Loss[cat1]: 0.006984\n",
      "train Loss[cat2]: 0.008564\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014970\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3571/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004478\n",
      "train Loss[cat1]: 0.006984\n",
      "train Loss[cat2]: 0.008563\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014970\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3572/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004478\n",
      "train Loss[cat1]: 0.006984\n",
      "train Loss[cat2]: 0.008563\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014970\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3573/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004477\n",
      "train Loss[cat1]: 0.006983\n",
      "train Loss[cat2]: 0.008563\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014970\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3574/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004477\n",
      "train Loss[cat1]: 0.006983\n",
      "train Loss[cat2]: 0.008562\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014970\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3575/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004477\n",
      "train Loss[cat1]: 0.006983\n",
      "train Loss[cat2]: 0.008562\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014970\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3576/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004477\n",
      "train Loss[cat1]: 0.006982\n",
      "train Loss[cat2]: 0.008561\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014972\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3577/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004476\n",
      "train Loss[cat1]: 0.006982\n",
      "train Loss[cat2]: 0.008561\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014970\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3578/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004476\n",
      "train Loss[cat1]: 0.006982\n",
      "train Loss[cat2]: 0.008561\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014970\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3579/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004476\n",
      "train Loss[cat1]: 0.006982\n",
      "train Loss[cat2]: 0.008560\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014971\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3580/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004476\n",
      "train Loss[cat1]: 0.006981\n",
      "train Loss[cat2]: 0.008560\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014971\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3581/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004476\n",
      "train Loss[cat1]: 0.006981\n",
      "train Loss[cat2]: 0.008560\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014970\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3582/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004475\n",
      "train Loss[cat1]: 0.006981\n",
      "train Loss[cat2]: 0.008559\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014972\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3583/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004475\n",
      "train Loss[cat1]: 0.006980\n",
      "train Loss[cat2]: 0.008559\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014971\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3584/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004475\n",
      "train Loss[cat1]: 0.006980\n",
      "train Loss[cat2]: 0.008559\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014971\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3585/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004475\n",
      "train Loss[cat1]: 0.006980\n",
      "train Loss[cat2]: 0.008558\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014972\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3586/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004474\n",
      "train Loss[cat1]: 0.006979\n",
      "train Loss[cat2]: 0.008558\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014973\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3587/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004474\n",
      "train Loss[cat1]: 0.006979\n",
      "train Loss[cat2]: 0.008557\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014973\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3588/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004474\n",
      "train Loss[cat1]: 0.006979\n",
      "train Loss[cat2]: 0.008557\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014973\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3589/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004474\n",
      "train Loss[cat1]: 0.006979\n",
      "train Loss[cat2]: 0.008557\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014972\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3590/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004474\n",
      "train Loss[cat1]: 0.006978\n",
      "train Loss[cat2]: 0.008556\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014972\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3591/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004473\n",
      "train Loss[cat1]: 0.006978\n",
      "train Loss[cat2]: 0.008556\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014972\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3592/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004473\n",
      "train Loss[cat1]: 0.006978\n",
      "train Loss[cat2]: 0.008556\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014971\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3593/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004473\n",
      "train Loss[cat1]: 0.006977\n",
      "train Loss[cat2]: 0.008555\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014971\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3594/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004473\n",
      "train Loss[cat1]: 0.006977\n",
      "train Loss[cat2]: 0.008555\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014973\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3595/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004473\n",
      "train Loss[cat1]: 0.006977\n",
      "train Loss[cat2]: 0.008555\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014972\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3596/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004473\n",
      "train Loss[cat1]: 0.006977\n",
      "train Loss[cat2]: 0.008554\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014971\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3597/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004472\n",
      "train Loss[cat1]: 0.006976\n",
      "train Loss[cat2]: 0.008554\n",
      "val Loss[cat0]: 0.009782\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3598/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004472\n",
      "train Loss[cat1]: 0.006976\n",
      "train Loss[cat2]: 0.008554\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3599/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004472\n",
      "train Loss[cat1]: 0.006976\n",
      "train Loss[cat2]: 0.008553\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3600/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004472\n",
      "train Loss[cat1]: 0.006975\n",
      "train Loss[cat2]: 0.008553\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3601/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004472\n",
      "train Loss[cat1]: 0.006975\n",
      "train Loss[cat2]: 0.008552\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3602/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004471\n",
      "train Loss[cat1]: 0.006975\n",
      "train Loss[cat2]: 0.008552\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3603/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004471\n",
      "train Loss[cat1]: 0.006974\n",
      "train Loss[cat2]: 0.008552\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3604/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004471\n",
      "train Loss[cat1]: 0.006974\n",
      "train Loss[cat2]: 0.008551\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3605/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004471\n",
      "train Loss[cat1]: 0.006974\n",
      "train Loss[cat2]: 0.008551\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3606/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004471\n",
      "train Loss[cat1]: 0.006974\n",
      "train Loss[cat2]: 0.008551\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3607/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004470\n",
      "train Loss[cat1]: 0.006973\n",
      "train Loss[cat2]: 0.008550\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3608/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004470\n",
      "train Loss[cat1]: 0.006973\n",
      "train Loss[cat2]: 0.008550\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3609/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004470\n",
      "train Loss[cat1]: 0.006973\n",
      "train Loss[cat2]: 0.008550\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3610/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004470\n",
      "train Loss[cat1]: 0.006973\n",
      "train Loss[cat2]: 0.008549\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3611/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004469\n",
      "train Loss[cat1]: 0.006972\n",
      "train Loss[cat2]: 0.008549\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3612/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004469\n",
      "train Loss[cat1]: 0.006972\n",
      "train Loss[cat2]: 0.008549\n",
      "val Loss[cat0]: 0.009779\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3613/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004469\n",
      "train Loss[cat1]: 0.006972\n",
      "train Loss[cat2]: 0.008548\n",
      "val Loss[cat0]: 0.009779\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3614/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004469\n",
      "train Loss[cat1]: 0.006971\n",
      "train Loss[cat2]: 0.008548\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3615/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004469\n",
      "train Loss[cat1]: 0.006971\n",
      "train Loss[cat2]: 0.008547\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3616/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004468\n",
      "train Loss[cat1]: 0.006971\n",
      "train Loss[cat2]: 0.008547\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3617/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004468\n",
      "train Loss[cat1]: 0.006970\n",
      "train Loss[cat2]: 0.008547\n",
      "val Loss[cat0]: 0.009779\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3618/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004468\n",
      "train Loss[cat1]: 0.006970\n",
      "train Loss[cat2]: 0.008546\n",
      "val Loss[cat0]: 0.009779\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3619/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004468\n",
      "train Loss[cat1]: 0.006970\n",
      "train Loss[cat2]: 0.008546\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3620/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004468\n",
      "train Loss[cat1]: 0.006970\n",
      "train Loss[cat2]: 0.008546\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3621/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004467\n",
      "train Loss[cat1]: 0.006969\n",
      "train Loss[cat2]: 0.008545\n",
      "val Loss[cat0]: 0.009779\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3622/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004467\n",
      "train Loss[cat1]: 0.006969\n",
      "train Loss[cat2]: 0.008545\n",
      "val Loss[cat0]: 0.009779\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3623/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004467\n",
      "train Loss[cat1]: 0.006969\n",
      "train Loss[cat2]: 0.008545\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3624/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004467\n",
      "train Loss[cat1]: 0.006968\n",
      "train Loss[cat2]: 0.008544\n",
      "val Loss[cat0]: 0.009779\n",
      "val Loss[cat1]: 0.014973\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3625/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004467\n",
      "train Loss[cat1]: 0.006968\n",
      "train Loss[cat2]: 0.008544\n",
      "val Loss[cat0]: 0.009779\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3626/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004467\n",
      "train Loss[cat1]: 0.006968\n",
      "train Loss[cat2]: 0.008544\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3627/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004466\n",
      "train Loss[cat1]: 0.006968\n",
      "train Loss[cat2]: 0.008543\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3628/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004466\n",
      "train Loss[cat1]: 0.006967\n",
      "train Loss[cat2]: 0.008543\n",
      "val Loss[cat0]: 0.009779\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3629/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004466\n",
      "train Loss[cat1]: 0.006967\n",
      "train Loss[cat2]: 0.008543\n",
      "val Loss[cat0]: 0.009781\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3630/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004466\n",
      "train Loss[cat1]: 0.006967\n",
      "train Loss[cat2]: 0.008542\n",
      "val Loss[cat0]: 0.009780\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3631/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004465\n",
      "train Loss[cat1]: 0.006966\n",
      "train Loss[cat2]: 0.008542\n",
      "val Loss[cat0]: 0.009779\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3632/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004465\n",
      "train Loss[cat1]: 0.006966\n",
      "train Loss[cat2]: 0.008541\n",
      "val Loss[cat0]: 0.009779\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3633/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004465\n",
      "train Loss[cat1]: 0.006966\n",
      "train Loss[cat2]: 0.008541\n",
      "val Loss[cat0]: 0.009778\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3634/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004465\n",
      "train Loss[cat1]: 0.006965\n",
      "train Loss[cat2]: 0.008541\n",
      "val Loss[cat0]: 0.009779\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3635/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004465\n",
      "train Loss[cat1]: 0.006965\n",
      "train Loss[cat2]: 0.008540\n",
      "val Loss[cat0]: 0.009778\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3636/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004464\n",
      "train Loss[cat1]: 0.006965\n",
      "train Loss[cat2]: 0.008540\n",
      "val Loss[cat0]: 0.009778\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3637/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004464\n",
      "train Loss[cat1]: 0.006965\n",
      "train Loss[cat2]: 0.008540\n",
      "val Loss[cat0]: 0.009778\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3638/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004464\n",
      "train Loss[cat1]: 0.006964\n",
      "train Loss[cat2]: 0.008539\n",
      "val Loss[cat0]: 0.009779\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3639/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004464\n",
      "train Loss[cat1]: 0.006964\n",
      "train Loss[cat2]: 0.008539\n",
      "val Loss[cat0]: 0.009778\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3640/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004464\n",
      "train Loss[cat1]: 0.006964\n",
      "train Loss[cat2]: 0.008539\n",
      "val Loss[cat0]: 0.009779\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3641/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004464\n",
      "train Loss[cat1]: 0.006964\n",
      "train Loss[cat2]: 0.008538\n",
      "val Loss[cat0]: 0.009779\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3642/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004463\n",
      "train Loss[cat1]: 0.006963\n",
      "train Loss[cat2]: 0.008538\n",
      "val Loss[cat0]: 0.009778\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3643/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004463\n",
      "train Loss[cat1]: 0.006963\n",
      "train Loss[cat2]: 0.008538\n",
      "val Loss[cat0]: 0.009778\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3644/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004463\n",
      "train Loss[cat1]: 0.006962\n",
      "train Loss[cat2]: 0.008537\n",
      "val Loss[cat0]: 0.009778\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3645/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004463\n",
      "train Loss[cat1]: 0.006962\n",
      "train Loss[cat2]: 0.008537\n",
      "val Loss[cat0]: 0.009777\n",
      "val Loss[cat1]: 0.014973\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3646/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004463\n",
      "train Loss[cat1]: 0.006962\n",
      "train Loss[cat2]: 0.008537\n",
      "val Loss[cat0]: 0.009778\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3647/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004462\n",
      "train Loss[cat1]: 0.006962\n",
      "train Loss[cat2]: 0.008536\n",
      "val Loss[cat0]: 0.009777\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3648/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004462\n",
      "train Loss[cat1]: 0.006961\n",
      "train Loss[cat2]: 0.008536\n",
      "val Loss[cat0]: 0.009779\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3649/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004462\n",
      "train Loss[cat1]: 0.006961\n",
      "train Loss[cat2]: 0.008535\n",
      "val Loss[cat0]: 0.009779\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3650/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004462\n",
      "train Loss[cat1]: 0.006961\n",
      "train Loss[cat2]: 0.008535\n",
      "val Loss[cat0]: 0.009778\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3651/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004462\n",
      "train Loss[cat1]: 0.006961\n",
      "train Loss[cat2]: 0.008535\n",
      "val Loss[cat0]: 0.009778\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3652/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004462\n",
      "train Loss[cat1]: 0.006960\n",
      "train Loss[cat2]: 0.008535\n",
      "val Loss[cat0]: 0.009778\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3653/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004461\n",
      "train Loss[cat1]: 0.006960\n",
      "train Loss[cat2]: 0.008534\n",
      "val Loss[cat0]: 0.009779\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3654/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004461\n",
      "train Loss[cat1]: 0.006960\n",
      "train Loss[cat2]: 0.008534\n",
      "val Loss[cat0]: 0.009779\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3655/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004461\n",
      "train Loss[cat1]: 0.006959\n",
      "train Loss[cat2]: 0.008533\n",
      "val Loss[cat0]: 0.009779\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3656/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004461\n",
      "train Loss[cat1]: 0.006959\n",
      "train Loss[cat2]: 0.008533\n",
      "val Loss[cat0]: 0.009778\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3657/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004461\n",
      "train Loss[cat1]: 0.006959\n",
      "train Loss[cat2]: 0.008533\n",
      "val Loss[cat0]: 0.009778\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3658/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004460\n",
      "train Loss[cat1]: 0.006958\n",
      "train Loss[cat2]: 0.008532\n",
      "val Loss[cat0]: 0.009778\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3659/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004460\n",
      "train Loss[cat1]: 0.006958\n",
      "train Loss[cat2]: 0.008532\n",
      "val Loss[cat0]: 0.009778\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3660/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004460\n",
      "train Loss[cat1]: 0.006958\n",
      "train Loss[cat2]: 0.008532\n",
      "val Loss[cat0]: 0.009777\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3661/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004460\n",
      "train Loss[cat1]: 0.006958\n",
      "train Loss[cat2]: 0.008531\n",
      "val Loss[cat0]: 0.009779\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3662/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004460\n",
      "train Loss[cat1]: 0.006957\n",
      "train Loss[cat2]: 0.008531\n",
      "val Loss[cat0]: 0.009778\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3663/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004459\n",
      "train Loss[cat1]: 0.006957\n",
      "train Loss[cat2]: 0.008530\n",
      "val Loss[cat0]: 0.009778\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3664/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004459\n",
      "train Loss[cat1]: 0.006957\n",
      "train Loss[cat2]: 0.008530\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3665/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004459\n",
      "train Loss[cat1]: 0.006956\n",
      "train Loss[cat2]: 0.008530\n",
      "val Loss[cat0]: 0.009777\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3666/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004459\n",
      "train Loss[cat1]: 0.006956\n",
      "train Loss[cat2]: 0.008529\n",
      "val Loss[cat0]: 0.009777\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3667/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004459\n",
      "train Loss[cat1]: 0.006956\n",
      "train Loss[cat2]: 0.008529\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3668/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004458\n",
      "train Loss[cat1]: 0.006956\n",
      "train Loss[cat2]: 0.008529\n",
      "val Loss[cat0]: 0.009778\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3669/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004458\n",
      "train Loss[cat1]: 0.006955\n",
      "train Loss[cat2]: 0.008528\n",
      "val Loss[cat0]: 0.009777\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3670/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004458\n",
      "train Loss[cat1]: 0.006955\n",
      "train Loss[cat2]: 0.008528\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3671/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004458\n",
      "train Loss[cat1]: 0.006955\n",
      "train Loss[cat2]: 0.008528\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3672/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004458\n",
      "train Loss[cat1]: 0.006954\n",
      "train Loss[cat2]: 0.008527\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3673/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004457\n",
      "train Loss[cat1]: 0.006954\n",
      "train Loss[cat2]: 0.008527\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3674/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004457\n",
      "train Loss[cat1]: 0.006954\n",
      "train Loss[cat2]: 0.008527\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3675/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004457\n",
      "train Loss[cat1]: 0.006954\n",
      "train Loss[cat2]: 0.008526\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3676/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004457\n",
      "train Loss[cat1]: 0.006953\n",
      "train Loss[cat2]: 0.008526\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3677/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004457\n",
      "train Loss[cat1]: 0.006953\n",
      "train Loss[cat2]: 0.008526\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3678/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004457\n",
      "train Loss[cat1]: 0.006953\n",
      "train Loss[cat2]: 0.008525\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3679/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004456\n",
      "train Loss[cat1]: 0.006953\n",
      "train Loss[cat2]: 0.008525\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3680/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004456\n",
      "train Loss[cat1]: 0.006952\n",
      "train Loss[cat2]: 0.008525\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3681/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004456\n",
      "train Loss[cat1]: 0.006952\n",
      "train Loss[cat2]: 0.008524\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3682/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004456\n",
      "train Loss[cat1]: 0.006952\n",
      "train Loss[cat2]: 0.008524\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3683/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004455\n",
      "train Loss[cat1]: 0.006951\n",
      "train Loss[cat2]: 0.008524\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3684/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004455\n",
      "train Loss[cat1]: 0.006951\n",
      "train Loss[cat2]: 0.008523\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3685/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004455\n",
      "train Loss[cat1]: 0.006951\n",
      "train Loss[cat2]: 0.008523\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3686/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004455\n",
      "train Loss[cat1]: 0.006950\n",
      "train Loss[cat2]: 0.008522\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3687/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004455\n",
      "train Loss[cat1]: 0.006950\n",
      "train Loss[cat2]: 0.008522\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3688/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004454\n",
      "train Loss[cat1]: 0.006950\n",
      "train Loss[cat2]: 0.008522\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3689/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004454\n",
      "train Loss[cat1]: 0.006950\n",
      "train Loss[cat2]: 0.008521\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3690/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004454\n",
      "train Loss[cat1]: 0.006949\n",
      "train Loss[cat2]: 0.008521\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3691/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004454\n",
      "train Loss[cat1]: 0.006949\n",
      "train Loss[cat2]: 0.008521\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3692/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004454\n",
      "train Loss[cat1]: 0.006949\n",
      "train Loss[cat2]: 0.008520\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3693/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004453\n",
      "train Loss[cat1]: 0.006948\n",
      "train Loss[cat2]: 0.008520\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3694/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004453\n",
      "train Loss[cat1]: 0.006948\n",
      "train Loss[cat2]: 0.008520\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3695/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004453\n",
      "train Loss[cat1]: 0.006948\n",
      "train Loss[cat2]: 0.008519\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3696/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004453\n",
      "train Loss[cat1]: 0.006948\n",
      "train Loss[cat2]: 0.008519\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3697/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004453\n",
      "train Loss[cat1]: 0.006947\n",
      "train Loss[cat2]: 0.008519\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3698/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004453\n",
      "train Loss[cat1]: 0.006947\n",
      "train Loss[cat2]: 0.008518\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3699/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004452\n",
      "train Loss[cat1]: 0.006947\n",
      "train Loss[cat2]: 0.008518\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3700/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004452\n",
      "train Loss[cat1]: 0.006946\n",
      "train Loss[cat2]: 0.008518\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3701/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004452\n",
      "train Loss[cat1]: 0.006946\n",
      "train Loss[cat2]: 0.008517\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3702/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004452\n",
      "train Loss[cat1]: 0.006946\n",
      "train Loss[cat2]: 0.008517\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3703/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004452\n",
      "train Loss[cat1]: 0.006945\n",
      "train Loss[cat2]: 0.008517\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3704/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004451\n",
      "train Loss[cat1]: 0.006945\n",
      "train Loss[cat2]: 0.008516\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3705/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004451\n",
      "train Loss[cat1]: 0.006945\n",
      "train Loss[cat2]: 0.008516\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3706/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004451\n",
      "train Loss[cat1]: 0.006945\n",
      "train Loss[cat2]: 0.008516\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3707/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004451\n",
      "train Loss[cat1]: 0.006944\n",
      "train Loss[cat2]: 0.008515\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3708/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004451\n",
      "train Loss[cat1]: 0.006944\n",
      "train Loss[cat2]: 0.008515\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3709/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004450\n",
      "train Loss[cat1]: 0.006944\n",
      "train Loss[cat2]: 0.008514\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3710/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004450\n",
      "train Loss[cat1]: 0.006943\n",
      "train Loss[cat2]: 0.008514\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3711/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004450\n",
      "train Loss[cat1]: 0.006943\n",
      "train Loss[cat2]: 0.008514\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3712/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004450\n",
      "train Loss[cat1]: 0.006943\n",
      "train Loss[cat2]: 0.008513\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3713/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004450\n",
      "train Loss[cat1]: 0.006943\n",
      "train Loss[cat2]: 0.008513\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3714/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004449\n",
      "train Loss[cat1]: 0.006942\n",
      "train Loss[cat2]: 0.008513\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3715/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004449\n",
      "train Loss[cat1]: 0.006942\n",
      "train Loss[cat2]: 0.008512\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3716/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004449\n",
      "train Loss[cat1]: 0.006942\n",
      "train Loss[cat2]: 0.008512\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3717/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004449\n",
      "train Loss[cat1]: 0.006942\n",
      "train Loss[cat2]: 0.008511\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3718/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004449\n",
      "train Loss[cat1]: 0.006941\n",
      "train Loss[cat2]: 0.008511\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3719/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004449\n",
      "train Loss[cat1]: 0.006941\n",
      "train Loss[cat2]: 0.008511\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3720/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004448\n",
      "train Loss[cat1]: 0.006941\n",
      "train Loss[cat2]: 0.008510\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3721/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004448\n",
      "train Loss[cat1]: 0.006940\n",
      "train Loss[cat2]: 0.008510\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3722/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004448\n",
      "train Loss[cat1]: 0.006940\n",
      "train Loss[cat2]: 0.008510\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3723/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004448\n",
      "train Loss[cat1]: 0.006940\n",
      "train Loss[cat2]: 0.008509\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3724/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004448\n",
      "train Loss[cat1]: 0.006939\n",
      "train Loss[cat2]: 0.008509\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3725/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004448\n",
      "train Loss[cat1]: 0.006939\n",
      "train Loss[cat2]: 0.008509\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3726/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004447\n",
      "train Loss[cat1]: 0.006939\n",
      "train Loss[cat2]: 0.008508\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3727/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004447\n",
      "train Loss[cat1]: 0.006939\n",
      "train Loss[cat2]: 0.008508\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3728/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004447\n",
      "train Loss[cat1]: 0.006938\n",
      "train Loss[cat2]: 0.008508\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3729/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004447\n",
      "train Loss[cat1]: 0.006938\n",
      "train Loss[cat2]: 0.008507\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3730/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004447\n",
      "train Loss[cat1]: 0.006938\n",
      "train Loss[cat2]: 0.008507\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3731/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004446\n",
      "train Loss[cat1]: 0.006937\n",
      "train Loss[cat2]: 0.008507\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3732/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004446\n",
      "train Loss[cat1]: 0.006937\n",
      "train Loss[cat2]: 0.008506\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3733/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004446\n",
      "train Loss[cat1]: 0.006937\n",
      "train Loss[cat2]: 0.008506\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3734/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004446\n",
      "train Loss[cat1]: 0.006936\n",
      "train Loss[cat2]: 0.008506\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3735/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004446\n",
      "train Loss[cat1]: 0.006936\n",
      "train Loss[cat2]: 0.008505\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3736/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004445\n",
      "train Loss[cat1]: 0.006936\n",
      "train Loss[cat2]: 0.008505\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3737/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004445\n",
      "train Loss[cat1]: 0.006936\n",
      "train Loss[cat2]: 0.008505\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3738/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004445\n",
      "train Loss[cat1]: 0.006935\n",
      "train Loss[cat2]: 0.008504\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3739/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004445\n",
      "train Loss[cat1]: 0.006935\n",
      "train Loss[cat2]: 0.008504\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3740/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004445\n",
      "train Loss[cat1]: 0.006935\n",
      "train Loss[cat2]: 0.008504\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3741/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004445\n",
      "train Loss[cat1]: 0.006934\n",
      "train Loss[cat2]: 0.008503\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3742/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004444\n",
      "train Loss[cat1]: 0.006934\n",
      "train Loss[cat2]: 0.008503\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3743/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004444\n",
      "train Loss[cat1]: 0.006934\n",
      "train Loss[cat2]: 0.008503\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3744/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004444\n",
      "train Loss[cat1]: 0.006933\n",
      "train Loss[cat2]: 0.008502\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3745/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004444\n",
      "train Loss[cat1]: 0.006933\n",
      "train Loss[cat2]: 0.008502\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3746/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004443\n",
      "train Loss[cat1]: 0.006933\n",
      "train Loss[cat2]: 0.008502\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3747/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004443\n",
      "train Loss[cat1]: 0.006933\n",
      "train Loss[cat2]: 0.008501\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3748/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004443\n",
      "train Loss[cat1]: 0.006932\n",
      "train Loss[cat2]: 0.008501\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3749/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004443\n",
      "train Loss[cat1]: 0.006932\n",
      "train Loss[cat2]: 0.008501\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3750/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004443\n",
      "train Loss[cat1]: 0.006932\n",
      "train Loss[cat2]: 0.008500\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3751/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004443\n",
      "train Loss[cat1]: 0.006932\n",
      "train Loss[cat2]: 0.008500\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3752/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004442\n",
      "train Loss[cat1]: 0.006931\n",
      "train Loss[cat2]: 0.008500\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3753/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004442\n",
      "train Loss[cat1]: 0.006931\n",
      "train Loss[cat2]: 0.008499\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3754/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004442\n",
      "train Loss[cat1]: 0.006931\n",
      "train Loss[cat2]: 0.008499\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3755/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004442\n",
      "train Loss[cat1]: 0.006930\n",
      "train Loss[cat2]: 0.008499\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3756/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004442\n",
      "train Loss[cat1]: 0.006930\n",
      "train Loss[cat2]: 0.008498\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3757/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004442\n",
      "train Loss[cat1]: 0.006930\n",
      "train Loss[cat2]: 0.008498\n",
      "val Loss[cat0]: 0.009770\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3758/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004441\n",
      "train Loss[cat1]: 0.006929\n",
      "train Loss[cat2]: 0.008498\n",
      "val Loss[cat0]: 0.009770\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3759/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004441\n",
      "train Loss[cat1]: 0.006929\n",
      "train Loss[cat2]: 0.008497\n",
      "val Loss[cat0]: 0.009770\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3760/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004441\n",
      "train Loss[cat1]: 0.006929\n",
      "train Loss[cat2]: 0.008497\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3761/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004441\n",
      "train Loss[cat1]: 0.006928\n",
      "train Loss[cat2]: 0.008496\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3762/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004441\n",
      "train Loss[cat1]: 0.006928\n",
      "train Loss[cat2]: 0.008496\n",
      "val Loss[cat0]: 0.009770\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3763/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004440\n",
      "train Loss[cat1]: 0.006928\n",
      "train Loss[cat2]: 0.008496\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3764/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004440\n",
      "train Loss[cat1]: 0.006928\n",
      "train Loss[cat2]: 0.008495\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3765/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004440\n",
      "train Loss[cat1]: 0.006927\n",
      "train Loss[cat2]: 0.008495\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3766/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004440\n",
      "train Loss[cat1]: 0.006927\n",
      "train Loss[cat2]: 0.008495\n",
      "val Loss[cat0]: 0.009770\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3767/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004440\n",
      "train Loss[cat1]: 0.006927\n",
      "train Loss[cat2]: 0.008494\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3768/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004440\n",
      "train Loss[cat1]: 0.006926\n",
      "train Loss[cat2]: 0.008494\n",
      "val Loss[cat0]: 0.009770\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3769/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004439\n",
      "train Loss[cat1]: 0.006926\n",
      "train Loss[cat2]: 0.008494\n",
      "val Loss[cat0]: 0.009770\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3770/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004439\n",
      "train Loss[cat1]: 0.006926\n",
      "train Loss[cat2]: 0.008493\n",
      "val Loss[cat0]: 0.009770\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3771/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004439\n",
      "train Loss[cat1]: 0.006926\n",
      "train Loss[cat2]: 0.008493\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3772/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004439\n",
      "train Loss[cat1]: 0.006925\n",
      "train Loss[cat2]: 0.008493\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3773/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004439\n",
      "train Loss[cat1]: 0.006925\n",
      "train Loss[cat2]: 0.008492\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3774/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004439\n",
      "train Loss[cat1]: 0.006925\n",
      "train Loss[cat2]: 0.008492\n",
      "val Loss[cat0]: 0.009769\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3775/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004438\n",
      "train Loss[cat1]: 0.006924\n",
      "train Loss[cat2]: 0.008492\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3776/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004438\n",
      "train Loss[cat1]: 0.006924\n",
      "train Loss[cat2]: 0.008491\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3777/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004438\n",
      "train Loss[cat1]: 0.006924\n",
      "train Loss[cat2]: 0.008491\n",
      "val Loss[cat0]: 0.009770\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3778/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004438\n",
      "train Loss[cat1]: 0.006924\n",
      "train Loss[cat2]: 0.008491\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3779/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004438\n",
      "train Loss[cat1]: 0.006923\n",
      "train Loss[cat2]: 0.008490\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3780/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004437\n",
      "train Loss[cat1]: 0.006923\n",
      "train Loss[cat2]: 0.008490\n",
      "val Loss[cat0]: 0.009770\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3781/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004437\n",
      "train Loss[cat1]: 0.006923\n",
      "train Loss[cat2]: 0.008490\n",
      "val Loss[cat0]: 0.009770\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3782/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004437\n",
      "train Loss[cat1]: 0.006922\n",
      "train Loss[cat2]: 0.008489\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3783/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004437\n",
      "train Loss[cat1]: 0.006922\n",
      "train Loss[cat2]: 0.008489\n",
      "val Loss[cat0]: 0.009770\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3784/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004437\n",
      "train Loss[cat1]: 0.006922\n",
      "train Loss[cat2]: 0.008489\n",
      "val Loss[cat0]: 0.009769\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3785/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004437\n",
      "train Loss[cat1]: 0.006921\n",
      "train Loss[cat2]: 0.008488\n",
      "val Loss[cat0]: 0.009770\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3786/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004436\n",
      "train Loss[cat1]: 0.006921\n",
      "train Loss[cat2]: 0.008488\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3787/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004436\n",
      "train Loss[cat1]: 0.006921\n",
      "train Loss[cat2]: 0.008488\n",
      "val Loss[cat0]: 0.009770\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3788/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004436\n",
      "train Loss[cat1]: 0.006921\n",
      "train Loss[cat2]: 0.008487\n",
      "val Loss[cat0]: 0.009770\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3789/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004436\n",
      "train Loss[cat1]: 0.006920\n",
      "train Loss[cat2]: 0.008487\n",
      "val Loss[cat0]: 0.009770\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3790/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004436\n",
      "train Loss[cat1]: 0.006920\n",
      "train Loss[cat2]: 0.008487\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3791/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004436\n",
      "train Loss[cat1]: 0.006920\n",
      "train Loss[cat2]: 0.008486\n",
      "val Loss[cat0]: 0.009770\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3792/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004435\n",
      "train Loss[cat1]: 0.006920\n",
      "train Loss[cat2]: 0.008486\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3793/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004435\n",
      "train Loss[cat1]: 0.006919\n",
      "train Loss[cat2]: 0.008486\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3794/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004435\n",
      "train Loss[cat1]: 0.006919\n",
      "train Loss[cat2]: 0.008485\n",
      "val Loss[cat0]: 0.009770\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3795/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004435\n",
      "train Loss[cat1]: 0.006919\n",
      "train Loss[cat2]: 0.008485\n",
      "val Loss[cat0]: 0.009769\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3796/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004434\n",
      "train Loss[cat1]: 0.006918\n",
      "train Loss[cat2]: 0.008485\n",
      "val Loss[cat0]: 0.009770\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3797/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004434\n",
      "train Loss[cat1]: 0.006918\n",
      "train Loss[cat2]: 0.008484\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3798/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004434\n",
      "train Loss[cat1]: 0.006918\n",
      "train Loss[cat2]: 0.008484\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3799/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004434\n",
      "train Loss[cat1]: 0.006918\n",
      "train Loss[cat2]: 0.008483\n",
      "val Loss[cat0]: 0.009770\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3800/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004434\n",
      "train Loss[cat1]: 0.006917\n",
      "train Loss[cat2]: 0.008483\n",
      "val Loss[cat0]: 0.009770\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3801/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004434\n",
      "train Loss[cat1]: 0.006917\n",
      "train Loss[cat2]: 0.008483\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3802/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004433\n",
      "train Loss[cat1]: 0.006917\n",
      "train Loss[cat2]: 0.008483\n",
      "val Loss[cat0]: 0.009770\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3803/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004433\n",
      "train Loss[cat1]: 0.006917\n",
      "train Loss[cat2]: 0.008482\n",
      "val Loss[cat0]: 0.009769\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3804/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004433\n",
      "train Loss[cat1]: 0.006916\n",
      "train Loss[cat2]: 0.008482\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3805/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004433\n",
      "train Loss[cat1]: 0.006916\n",
      "train Loss[cat2]: 0.008482\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3806/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004433\n",
      "train Loss[cat1]: 0.006916\n",
      "train Loss[cat2]: 0.008481\n",
      "val Loss[cat0]: 0.009770\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3807/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004433\n",
      "train Loss[cat1]: 0.006915\n",
      "train Loss[cat2]: 0.008481\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3808/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004433\n",
      "train Loss[cat1]: 0.006915\n",
      "train Loss[cat2]: 0.008480\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3809/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004432\n",
      "train Loss[cat1]: 0.006915\n",
      "train Loss[cat2]: 0.008480\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3810/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004432\n",
      "train Loss[cat1]: 0.006915\n",
      "train Loss[cat2]: 0.008480\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3811/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004432\n",
      "train Loss[cat1]: 0.006914\n",
      "train Loss[cat2]: 0.008479\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3812/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004432\n",
      "train Loss[cat1]: 0.006914\n",
      "train Loss[cat2]: 0.008479\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3813/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004432\n",
      "train Loss[cat1]: 0.006914\n",
      "train Loss[cat2]: 0.008479\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014981\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3814/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004431\n",
      "train Loss[cat1]: 0.006913\n",
      "train Loss[cat2]: 0.008478\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3815/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004431\n",
      "train Loss[cat1]: 0.006913\n",
      "train Loss[cat2]: 0.008478\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3816/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004431\n",
      "train Loss[cat1]: 0.006913\n",
      "train Loss[cat2]: 0.008478\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3817/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004431\n",
      "train Loss[cat1]: 0.006913\n",
      "train Loss[cat2]: 0.008477\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3818/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004431\n",
      "train Loss[cat1]: 0.006912\n",
      "train Loss[cat2]: 0.008477\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3819/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004430\n",
      "train Loss[cat1]: 0.006912\n",
      "train Loss[cat2]: 0.008477\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3820/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004430\n",
      "train Loss[cat1]: 0.006912\n",
      "train Loss[cat2]: 0.008476\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3821/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004430\n",
      "train Loss[cat1]: 0.006912\n",
      "train Loss[cat2]: 0.008476\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3822/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004430\n",
      "train Loss[cat1]: 0.006911\n",
      "train Loss[cat2]: 0.008476\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3823/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004430\n",
      "train Loss[cat1]: 0.006911\n",
      "train Loss[cat2]: 0.008475\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3824/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004430\n",
      "train Loss[cat1]: 0.006911\n",
      "train Loss[cat2]: 0.008475\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3825/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004429\n",
      "train Loss[cat1]: 0.006910\n",
      "train Loss[cat2]: 0.008475\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3826/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004429\n",
      "train Loss[cat1]: 0.006910\n",
      "train Loss[cat2]: 0.008475\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3827/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004429\n",
      "train Loss[cat1]: 0.006910\n",
      "train Loss[cat2]: 0.008474\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3828/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004429\n",
      "train Loss[cat1]: 0.006910\n",
      "train Loss[cat2]: 0.008474\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3829/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004429\n",
      "train Loss[cat1]: 0.006909\n",
      "train Loss[cat2]: 0.008474\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3830/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004428\n",
      "train Loss[cat1]: 0.006909\n",
      "train Loss[cat2]: 0.008473\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3831/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004428\n",
      "train Loss[cat1]: 0.006909\n",
      "train Loss[cat2]: 0.008473\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3832/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004428\n",
      "train Loss[cat1]: 0.006908\n",
      "train Loss[cat2]: 0.008473\n",
      "val Loss[cat0]: 0.009771\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3833/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004428\n",
      "train Loss[cat1]: 0.006908\n",
      "train Loss[cat2]: 0.008472\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3834/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004428\n",
      "train Loss[cat1]: 0.006908\n",
      "train Loss[cat2]: 0.008472\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3835/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004428\n",
      "train Loss[cat1]: 0.006908\n",
      "train Loss[cat2]: 0.008472\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3836/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004428\n",
      "train Loss[cat1]: 0.006907\n",
      "train Loss[cat2]: 0.008471\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3837/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004428\n",
      "train Loss[cat1]: 0.006907\n",
      "train Loss[cat2]: 0.008471\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3838/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004427\n",
      "train Loss[cat1]: 0.006907\n",
      "train Loss[cat2]: 0.008471\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3839/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004427\n",
      "train Loss[cat1]: 0.006906\n",
      "train Loss[cat2]: 0.008470\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3840/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004427\n",
      "train Loss[cat1]: 0.006906\n",
      "train Loss[cat2]: 0.008470\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3841/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004427\n",
      "train Loss[cat1]: 0.006906\n",
      "train Loss[cat2]: 0.008470\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3842/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004427\n",
      "train Loss[cat1]: 0.006906\n",
      "train Loss[cat2]: 0.008469\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3843/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004426\n",
      "train Loss[cat1]: 0.006905\n",
      "train Loss[cat2]: 0.008469\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3844/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004426\n",
      "train Loss[cat1]: 0.006905\n",
      "train Loss[cat2]: 0.008469\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3845/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004426\n",
      "train Loss[cat1]: 0.006905\n",
      "train Loss[cat2]: 0.008469\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3846/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004426\n",
      "train Loss[cat1]: 0.006905\n",
      "train Loss[cat2]: 0.008468\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3847/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004426\n",
      "train Loss[cat1]: 0.006904\n",
      "train Loss[cat2]: 0.008468\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3848/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004426\n",
      "train Loss[cat1]: 0.006904\n",
      "train Loss[cat2]: 0.008468\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3849/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004425\n",
      "train Loss[cat1]: 0.006904\n",
      "train Loss[cat2]: 0.008467\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3850/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004425\n",
      "train Loss[cat1]: 0.006904\n",
      "train Loss[cat2]: 0.008467\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3851/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004425\n",
      "train Loss[cat1]: 0.006903\n",
      "train Loss[cat2]: 0.008467\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3852/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004425\n",
      "train Loss[cat1]: 0.006903\n",
      "train Loss[cat2]: 0.008466\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3853/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004425\n",
      "train Loss[cat1]: 0.006903\n",
      "train Loss[cat2]: 0.008466\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014980\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3854/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004424\n",
      "train Loss[cat1]: 0.006902\n",
      "train Loss[cat2]: 0.008466\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014980\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3855/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004424\n",
      "train Loss[cat1]: 0.006902\n",
      "train Loss[cat2]: 0.008465\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3856/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004424\n",
      "train Loss[cat1]: 0.006902\n",
      "train Loss[cat2]: 0.008465\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3857/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004424\n",
      "train Loss[cat1]: 0.006902\n",
      "train Loss[cat2]: 0.008465\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3858/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004424\n",
      "train Loss[cat1]: 0.006901\n",
      "train Loss[cat2]: 0.008464\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3859/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004424\n",
      "train Loss[cat1]: 0.006901\n",
      "train Loss[cat2]: 0.008464\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014980\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3860/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004423\n",
      "train Loss[cat1]: 0.006901\n",
      "train Loss[cat2]: 0.008464\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3861/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004423\n",
      "train Loss[cat1]: 0.006901\n",
      "train Loss[cat2]: 0.008464\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3862/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004423\n",
      "train Loss[cat1]: 0.006900\n",
      "train Loss[cat2]: 0.008463\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3863/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004423\n",
      "train Loss[cat1]: 0.006900\n",
      "train Loss[cat2]: 0.008463\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3864/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004423\n",
      "train Loss[cat1]: 0.006900\n",
      "train Loss[cat2]: 0.008463\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3865/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004423\n",
      "train Loss[cat1]: 0.006899\n",
      "train Loss[cat2]: 0.008462\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3866/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004422\n",
      "train Loss[cat1]: 0.006899\n",
      "train Loss[cat2]: 0.008462\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3867/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004422\n",
      "train Loss[cat1]: 0.006899\n",
      "train Loss[cat2]: 0.008462\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3868/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004422\n",
      "train Loss[cat1]: 0.006899\n",
      "train Loss[cat2]: 0.008461\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3869/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004422\n",
      "train Loss[cat1]: 0.006898\n",
      "train Loss[cat2]: 0.008461\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3870/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004422\n",
      "train Loss[cat1]: 0.006898\n",
      "train Loss[cat2]: 0.008461\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014980\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3871/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004422\n",
      "train Loss[cat1]: 0.006898\n",
      "train Loss[cat2]: 0.008460\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3872/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004421\n",
      "train Loss[cat1]: 0.006898\n",
      "train Loss[cat2]: 0.008460\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3873/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004421\n",
      "train Loss[cat1]: 0.006897\n",
      "train Loss[cat2]: 0.008460\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014980\n",
      "val Loss[cat2]: 0.018548\n",
      "\n",
      "Epoch 3874/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004421\n",
      "train Loss[cat1]: 0.006897\n",
      "train Loss[cat2]: 0.008459\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014980\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3875/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004421\n",
      "train Loss[cat1]: 0.006897\n",
      "train Loss[cat2]: 0.008459\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3876/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004421\n",
      "train Loss[cat1]: 0.006896\n",
      "train Loss[cat2]: 0.008459\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014980\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3877/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004421\n",
      "train Loss[cat1]: 0.006896\n",
      "train Loss[cat2]: 0.008458\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014980\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3878/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004420\n",
      "train Loss[cat1]: 0.006896\n",
      "train Loss[cat2]: 0.008458\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3879/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004420\n",
      "train Loss[cat1]: 0.006896\n",
      "train Loss[cat2]: 0.008458\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014980\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3880/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004420\n",
      "train Loss[cat1]: 0.006895\n",
      "train Loss[cat2]: 0.008458\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3881/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004420\n",
      "train Loss[cat1]: 0.006895\n",
      "train Loss[cat2]: 0.008457\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3882/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004420\n",
      "train Loss[cat1]: 0.006895\n",
      "train Loss[cat2]: 0.008457\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3883/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004420\n",
      "train Loss[cat1]: 0.006895\n",
      "train Loss[cat2]: 0.008457\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3884/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004420\n",
      "train Loss[cat1]: 0.006894\n",
      "train Loss[cat2]: 0.008456\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3885/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004419\n",
      "train Loss[cat1]: 0.006894\n",
      "train Loss[cat2]: 0.008456\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3886/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004419\n",
      "train Loss[cat1]: 0.006894\n",
      "train Loss[cat2]: 0.008456\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3887/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004419\n",
      "train Loss[cat1]: 0.006894\n",
      "train Loss[cat2]: 0.008455\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014980\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3888/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004419\n",
      "train Loss[cat1]: 0.006893\n",
      "train Loss[cat2]: 0.008455\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3889/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004419\n",
      "train Loss[cat1]: 0.006893\n",
      "train Loss[cat2]: 0.008455\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3890/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004418\n",
      "train Loss[cat1]: 0.006893\n",
      "train Loss[cat2]: 0.008454\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014980\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3891/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004418\n",
      "train Loss[cat1]: 0.006893\n",
      "train Loss[cat2]: 0.008454\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3892/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004418\n",
      "train Loss[cat1]: 0.006892\n",
      "train Loss[cat2]: 0.008454\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3893/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004418\n",
      "train Loss[cat1]: 0.006892\n",
      "train Loss[cat2]: 0.008454\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3894/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004418\n",
      "train Loss[cat1]: 0.006892\n",
      "train Loss[cat2]: 0.008453\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3895/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004418\n",
      "train Loss[cat1]: 0.006892\n",
      "train Loss[cat2]: 0.008453\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014980\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3896/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004417\n",
      "train Loss[cat1]: 0.006891\n",
      "train Loss[cat2]: 0.008453\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3897/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004417\n",
      "train Loss[cat1]: 0.006891\n",
      "train Loss[cat2]: 0.008452\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3898/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004417\n",
      "train Loss[cat1]: 0.006891\n",
      "train Loss[cat2]: 0.008452\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3899/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004417\n",
      "train Loss[cat1]: 0.006890\n",
      "train Loss[cat2]: 0.008452\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3900/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004417\n",
      "train Loss[cat1]: 0.006890\n",
      "train Loss[cat2]: 0.008451\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3901/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004417\n",
      "train Loss[cat1]: 0.006890\n",
      "train Loss[cat2]: 0.008451\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3902/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004417\n",
      "train Loss[cat1]: 0.006890\n",
      "train Loss[cat2]: 0.008451\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3903/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004416\n",
      "train Loss[cat1]: 0.006889\n",
      "train Loss[cat2]: 0.008451\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3904/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004416\n",
      "train Loss[cat1]: 0.006889\n",
      "train Loss[cat2]: 0.008450\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3905/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004416\n",
      "train Loss[cat1]: 0.006889\n",
      "train Loss[cat2]: 0.008450\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3906/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004416\n",
      "train Loss[cat1]: 0.006889\n",
      "train Loss[cat2]: 0.008450\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3907/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004416\n",
      "train Loss[cat1]: 0.006888\n",
      "train Loss[cat2]: 0.008449\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3908/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004415\n",
      "train Loss[cat1]: 0.006888\n",
      "train Loss[cat2]: 0.008449\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3909/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004415\n",
      "train Loss[cat1]: 0.006888\n",
      "train Loss[cat2]: 0.008449\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014980\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3910/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004415\n",
      "train Loss[cat1]: 0.006887\n",
      "train Loss[cat2]: 0.008448\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018548\n",
      "\n",
      "Epoch 3911/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004415\n",
      "train Loss[cat1]: 0.006887\n",
      "train Loss[cat2]: 0.008448\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3912/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004415\n",
      "train Loss[cat1]: 0.006887\n",
      "train Loss[cat2]: 0.008448\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3913/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004415\n",
      "train Loss[cat1]: 0.006887\n",
      "train Loss[cat2]: 0.008447\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3914/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004415\n",
      "train Loss[cat1]: 0.006886\n",
      "train Loss[cat2]: 0.008447\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3915/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004414\n",
      "train Loss[cat1]: 0.006886\n",
      "train Loss[cat2]: 0.008447\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3916/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004414\n",
      "train Loss[cat1]: 0.006886\n",
      "train Loss[cat2]: 0.008446\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3917/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004414\n",
      "train Loss[cat1]: 0.006886\n",
      "train Loss[cat2]: 0.008446\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3918/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004414\n",
      "train Loss[cat1]: 0.006885\n",
      "train Loss[cat2]: 0.008446\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3919/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004414\n",
      "train Loss[cat1]: 0.006885\n",
      "train Loss[cat2]: 0.008445\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3920/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004414\n",
      "train Loss[cat1]: 0.006885\n",
      "train Loss[cat2]: 0.008445\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3921/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004413\n",
      "train Loss[cat1]: 0.006885\n",
      "train Loss[cat2]: 0.008445\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3922/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004413\n",
      "train Loss[cat1]: 0.006884\n",
      "train Loss[cat2]: 0.008445\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3923/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004413\n",
      "train Loss[cat1]: 0.006884\n",
      "train Loss[cat2]: 0.008444\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3924/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004413\n",
      "train Loss[cat1]: 0.006884\n",
      "train Loss[cat2]: 0.008444\n",
      "val Loss[cat0]: 0.009777\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3925/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004413\n",
      "train Loss[cat1]: 0.006884\n",
      "train Loss[cat2]: 0.008444\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3926/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004413\n",
      "train Loss[cat1]: 0.006883\n",
      "train Loss[cat2]: 0.008443\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014979\n",
      "val Loss[cat2]: 0.018547\n",
      "\n",
      "Epoch 3927/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004412\n",
      "train Loss[cat1]: 0.006883\n",
      "train Loss[cat2]: 0.008443\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3928/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004412\n",
      "train Loss[cat1]: 0.006883\n",
      "train Loss[cat2]: 0.008443\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3929/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004412\n",
      "train Loss[cat1]: 0.006883\n",
      "train Loss[cat2]: 0.008442\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3930/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004412\n",
      "train Loss[cat1]: 0.006882\n",
      "train Loss[cat2]: 0.008442\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014978\n",
      "val Loss[cat2]: 0.018546\n",
      "\n",
      "Epoch 3931/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004412\n",
      "train Loss[cat1]: 0.006882\n",
      "train Loss[cat2]: 0.008442\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3932/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004411\n",
      "train Loss[cat1]: 0.006882\n",
      "train Loss[cat2]: 0.008442\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3933/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004411\n",
      "train Loss[cat1]: 0.006882\n",
      "train Loss[cat2]: 0.008441\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3934/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004411\n",
      "train Loss[cat1]: 0.006881\n",
      "train Loss[cat2]: 0.008441\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3935/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004411\n",
      "train Loss[cat1]: 0.006881\n",
      "train Loss[cat2]: 0.008441\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3936/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004411\n",
      "train Loss[cat1]: 0.006881\n",
      "train Loss[cat2]: 0.008440\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018545\n",
      "\n",
      "Epoch 3937/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004411\n",
      "train Loss[cat1]: 0.006880\n",
      "train Loss[cat2]: 0.008440\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3938/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004411\n",
      "train Loss[cat1]: 0.006880\n",
      "train Loss[cat2]: 0.008440\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014977\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3939/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004410\n",
      "train Loss[cat1]: 0.006880\n",
      "train Loss[cat2]: 0.008439\n",
      "val Loss[cat0]: 0.009776\n",
      "val Loss[cat1]: 0.014976\n",
      "val Loss[cat2]: 0.018544\n",
      "\n",
      "Epoch 3940/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004410\n",
      "train Loss[cat1]: 0.006880\n",
      "train Loss[cat2]: 0.008439\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3941/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004410\n",
      "train Loss[cat1]: 0.006880\n",
      "train Loss[cat2]: 0.008439\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3942/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004410\n",
      "train Loss[cat1]: 0.006879\n",
      "train Loss[cat2]: 0.008438\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3943/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004410\n",
      "train Loss[cat1]: 0.006879\n",
      "train Loss[cat2]: 0.008438\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3944/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004409\n",
      "train Loss[cat1]: 0.006879\n",
      "train Loss[cat2]: 0.008438\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3945/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004409\n",
      "train Loss[cat1]: 0.006878\n",
      "train Loss[cat2]: 0.008437\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3946/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004409\n",
      "train Loss[cat1]: 0.006878\n",
      "train Loss[cat2]: 0.008437\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3947/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004409\n",
      "train Loss[cat1]: 0.006878\n",
      "train Loss[cat2]: 0.008437\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014975\n",
      "val Loss[cat2]: 0.018543\n",
      "\n",
      "Epoch 3948/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004409\n",
      "train Loss[cat1]: 0.006878\n",
      "train Loss[cat2]: 0.008437\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3949/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004409\n",
      "train Loss[cat1]: 0.006877\n",
      "train Loss[cat2]: 0.008436\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3950/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004409\n",
      "train Loss[cat1]: 0.006877\n",
      "train Loss[cat2]: 0.008436\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014974\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3951/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004408\n",
      "train Loss[cat1]: 0.006877\n",
      "train Loss[cat2]: 0.008436\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014973\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3952/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004408\n",
      "train Loss[cat1]: 0.006877\n",
      "train Loss[cat2]: 0.008435\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014973\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3953/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004408\n",
      "train Loss[cat1]: 0.006876\n",
      "train Loss[cat2]: 0.008435\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014973\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3954/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004408\n",
      "train Loss[cat1]: 0.006876\n",
      "train Loss[cat2]: 0.008435\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014971\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3955/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004408\n",
      "train Loss[cat1]: 0.006876\n",
      "train Loss[cat2]: 0.008434\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014972\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3956/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004408\n",
      "train Loss[cat1]: 0.006876\n",
      "train Loss[cat2]: 0.008434\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014972\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3957/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004407\n",
      "train Loss[cat1]: 0.006875\n",
      "train Loss[cat2]: 0.008434\n",
      "val Loss[cat0]: 0.009772\n",
      "val Loss[cat1]: 0.014971\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3958/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004407\n",
      "train Loss[cat1]: 0.006875\n",
      "train Loss[cat2]: 0.008433\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014970\n",
      "val Loss[cat2]: 0.018539\n",
      "\n",
      "Epoch 3959/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004407\n",
      "train Loss[cat1]: 0.006875\n",
      "train Loss[cat2]: 0.008433\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014970\n",
      "val Loss[cat2]: 0.018539\n",
      "\n",
      "Epoch 3960/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004407\n",
      "train Loss[cat1]: 0.006875\n",
      "train Loss[cat2]: 0.008433\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014970\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3961/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004407\n",
      "train Loss[cat1]: 0.006874\n",
      "train Loss[cat2]: 0.008432\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014971\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3962/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004407\n",
      "train Loss[cat1]: 0.006874\n",
      "train Loss[cat2]: 0.008432\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014972\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3963/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004406\n",
      "train Loss[cat1]: 0.006874\n",
      "train Loss[cat2]: 0.008432\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014970\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3964/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004406\n",
      "train Loss[cat1]: 0.006874\n",
      "train Loss[cat2]: 0.008431\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014970\n",
      "val Loss[cat2]: 0.018539\n",
      "\n",
      "Epoch 3965/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004406\n",
      "train Loss[cat1]: 0.006873\n",
      "train Loss[cat2]: 0.008431\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014970\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3966/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004406\n",
      "train Loss[cat1]: 0.006873\n",
      "train Loss[cat2]: 0.008431\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014971\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3967/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004406\n",
      "train Loss[cat1]: 0.006873\n",
      "train Loss[cat2]: 0.008431\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014971\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3968/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004406\n",
      "train Loss[cat1]: 0.006873\n",
      "train Loss[cat2]: 0.008430\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014970\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3969/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004405\n",
      "train Loss[cat1]: 0.006872\n",
      "train Loss[cat2]: 0.008430\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014969\n",
      "val Loss[cat2]: 0.018539\n",
      "\n",
      "Epoch 3970/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004405\n",
      "train Loss[cat1]: 0.006872\n",
      "train Loss[cat2]: 0.008429\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014969\n",
      "val Loss[cat2]: 0.018538\n",
      "\n",
      "Epoch 3971/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004405\n",
      "train Loss[cat1]: 0.006872\n",
      "train Loss[cat2]: 0.008429\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014969\n",
      "val Loss[cat2]: 0.018539\n",
      "\n",
      "Epoch 3972/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004405\n",
      "train Loss[cat1]: 0.006871\n",
      "train Loss[cat2]: 0.008429\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014969\n",
      "val Loss[cat2]: 0.018539\n",
      "\n",
      "Epoch 3973/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004405\n",
      "train Loss[cat1]: 0.006872\n",
      "train Loss[cat2]: 0.008429\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014969\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3974/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004405\n",
      "train Loss[cat1]: 0.006871\n",
      "train Loss[cat2]: 0.008428\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014969\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3975/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004405\n",
      "train Loss[cat1]: 0.006871\n",
      "train Loss[cat2]: 0.008428\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014970\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3976/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004404\n",
      "train Loss[cat1]: 0.006871\n",
      "train Loss[cat2]: 0.008428\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014970\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3977/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004404\n",
      "train Loss[cat1]: 0.006870\n",
      "train Loss[cat2]: 0.008427\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014969\n",
      "val Loss[cat2]: 0.018539\n",
      "\n",
      "Epoch 3978/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004404\n",
      "train Loss[cat1]: 0.006870\n",
      "train Loss[cat2]: 0.008427\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014969\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3979/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004404\n",
      "train Loss[cat1]: 0.006870\n",
      "train Loss[cat2]: 0.008427\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014969\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3980/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004404\n",
      "train Loss[cat1]: 0.006870\n",
      "train Loss[cat2]: 0.008426\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014969\n",
      "val Loss[cat2]: 0.018539\n",
      "\n",
      "Epoch 3981/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004404\n",
      "train Loss[cat1]: 0.006869\n",
      "train Loss[cat2]: 0.008426\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014968\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3982/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004403\n",
      "train Loss[cat1]: 0.006869\n",
      "train Loss[cat2]: 0.008426\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014969\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3983/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004403\n",
      "train Loss[cat1]: 0.006869\n",
      "train Loss[cat2]: 0.008426\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014969\n",
      "val Loss[cat2]: 0.018540\n",
      "\n",
      "Epoch 3984/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004403\n",
      "train Loss[cat1]: 0.006869\n",
      "train Loss[cat2]: 0.008425\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014970\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3985/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004403\n",
      "train Loss[cat1]: 0.006869\n",
      "train Loss[cat2]: 0.008425\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014970\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3986/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004403\n",
      "train Loss[cat1]: 0.006868\n",
      "train Loss[cat2]: 0.008425\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014969\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3987/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004403\n",
      "train Loss[cat1]: 0.006868\n",
      "train Loss[cat2]: 0.008424\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014970\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3988/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004402\n",
      "train Loss[cat1]: 0.006867\n",
      "train Loss[cat2]: 0.008424\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014970\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3989/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004402\n",
      "train Loss[cat1]: 0.006867\n",
      "train Loss[cat2]: 0.008424\n",
      "val Loss[cat0]: 0.009775\n",
      "val Loss[cat1]: 0.014970\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3990/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004402\n",
      "train Loss[cat1]: 0.006867\n",
      "train Loss[cat2]: 0.008423\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014970\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3991/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004402\n",
      "train Loss[cat1]: 0.006867\n",
      "train Loss[cat2]: 0.008423\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014969\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3992/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004402\n",
      "train Loss[cat1]: 0.006867\n",
      "train Loss[cat2]: 0.008423\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014969\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3993/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004402\n",
      "train Loss[cat1]: 0.006866\n",
      "train Loss[cat2]: 0.008422\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014969\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3994/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004402\n",
      "train Loss[cat1]: 0.006866\n",
      "train Loss[cat2]: 0.008422\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014969\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3995/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004401\n",
      "train Loss[cat1]: 0.006866\n",
      "train Loss[cat2]: 0.008422\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014969\n",
      "val Loss[cat2]: 0.018541\n",
      "\n",
      "Epoch 3996/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004401\n",
      "train Loss[cat1]: 0.006866\n",
      "train Loss[cat2]: 0.008421\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014969\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3997/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004401\n",
      "train Loss[cat1]: 0.006865\n",
      "train Loss[cat2]: 0.008421\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014969\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3998/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004401\n",
      "train Loss[cat1]: 0.006865\n",
      "train Loss[cat2]: 0.008421\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014969\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 3999/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004401\n",
      "train Loss[cat1]: 0.006865\n",
      "train Loss[cat2]: 0.008421\n",
      "val Loss[cat0]: 0.009774\n",
      "val Loss[cat1]: 0.014969\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Epoch 4000/4000\n",
      "----------\n",
      "train Loss[cat0]: 0.004401\n",
      "train Loss[cat1]: 0.006864\n",
      "train Loss[cat2]: 0.008420\n",
      "val Loss[cat0]: 0.009773\n",
      "val Loss[cat1]: 0.014968\n",
      "val Loss[cat2]: 0.018542\n",
      "\n",
      "Training complete in 2m 38s\n"
     ]
    }
   ],
   "source": [
    "model, loss_history = train_model(model, criterion, optimizer, num_epochs=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Y_S2m7d90k-l",
    "outputId": "d6059cb3-95fe-48a6-b8aa-05bd4472189b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_loss_history(title, index, loss_history):\n",
    "    loss_cat_train = [l[index] for l in np.log10(loss_history['train'])]\n",
    "    loss_cat_val = [l[index] for l in np.log10(loss_history['val'])]\n",
    "    plt.plot(loss_cat_train, label=\"train\")\n",
    "    plt.plot(loss_cat_val, label=\"val\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_loss_history(\"Cat1\", 0, loss_history)\n",
    "plot_loss_history(\"Cat2\", 1, loss_history)\n",
    "plot_loss_history(\"Cat3\", 2, loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFle1QiA5VAK"
   },
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s_4een-G1Lkf",
    "outputId": "dcacd2d8-7eff-4c67-b0b8-a7225d1086c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyClassifier(\n",
       "  (in_layer): Linear(in_features=16, out_features=64, bias=True)\n",
       "  (bn0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (hidden1_layer): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (hidden2_layer): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (out1_layer): Linear(in_features=32, out_features=4, bias=True)\n",
       "  (out2_layer): Linear(in_features=36, out_features=7, bias=True)\n",
       "  (out3_layer): Linear(in_features=43, out_features=10, bias=True)\n",
       "  (soft_max): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6iEb93p5pjR"
   },
   "source": [
    "#### Calculate Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pJxtxjI6O48",
    "outputId": "3d2a4f91-e756-4a3b-b792-8555f2571521"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss[cat0]: 0.008367\n",
      "Test Loss[cat1]: 0.012395\n",
      "Test Loss[cat2]: 0.014868\n"
     ]
    }
   ],
   "source": [
    "test_loss = [0,0,0]\n",
    "for inputs, labels in dataloaders['test']:\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        loss = [None, None, None]\n",
    "        for i in range(3):\n",
    "            loss[i] = criterion[i](outputs[i], labels[:,i])\n",
    "            test_loss[i] += loss[i].item()\n",
    "for i in range(3):\n",
    "    test_loss[i] = test_loss[i]/dataset_sizes['test']\n",
    "for i in range(3):\n",
    "    print(f'Test Loss[cat{i}]: {test_loss[i]:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSXJO2E25YfF"
   },
   "source": [
    "#### Define a function to use the model directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "633F2wgNHR0V"
   },
   "outputs": [],
   "source": [
    "def get_cat(x):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        p1, p2, p3 = model(x)\n",
    "        _, out1 = torch.max(p1, dim = 1)\n",
    "        _, out2 = torch.max(p2, dim = 1)\n",
    "        _, out3 = torch.max(p3, dim = 1)\n",
    "    return out1, out2, out3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKEaC_S65g-S"
   },
   "source": [
    "#### Calculate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "mRA4zUxOwSxk"
   },
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "for inputs, labels in dataloaders['test']:\n",
    "    outputs = get_cat(inputs)\n",
    "    outputs = [outputs[0].detach().numpy(), outputs[1].detach().numpy(), outputs[2].detach().numpy()]\n",
    "    outputs = [[o1, o2, o3] for o1, o2, o3 in zip(outputs[0], outputs[1], outputs[2])]\n",
    "    y_true.extend(labels.detach().numpy())\n",
    "    y_pred.extend(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "NrpqbIwywbHF"
   },
   "outputs": [],
   "source": [
    "def accuracy(pred, true):\n",
    "    true_count = [0,0,0]\n",
    "    for p,t in zip(pred, true):\n",
    "        for i in range(3):\n",
    "            if p[i] == t[i]:\n",
    "                true_count[i] += 1\n",
    "    true_count = np.divide(true_count,len(pred))\n",
    "    return true_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mIckKGoW0PyO",
    "outputId": "d8f45434-2f07-48eb-d19c-45aebcac76b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy [cat1]: %65.03\n",
      "Accuracy [cat2]: %40.91\n",
      "Accuracy [cat3]: %33.22\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(pred=y_pred, true=y_true)\n",
    "for i in range(3):\n",
    "    print(f'Accuracy [cat{i+1}]: %{acc[i]*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRLhA2QH5ll3"
   },
   "source": [
    "#### Some sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rcdWba856UzP",
    "outputId": "5453d78a-fe1f-42c5-d052-0de93b4eee2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cat1] True: 1 \t Pred: 0 \t Eq: False\n",
      "[cat2] True: 0 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 8 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat3] True: 8 \t Pred: 8 \t Eq: True\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat3] True: 9 \t Pred: 8 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 0 \t Eq: False\n",
      "[cat2] True: 0 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 8 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat3] True: 8 \t Pred: 9 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 3 \t Eq: False\n",
      "[cat2] True: 5 \t Pred: 6 \t Eq: False\n",
      "[cat3] True: 7 \t Pred: 0 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 5 \t Pred: 0 \t Eq: False\n",
      "[cat3] True: 7 \t Pred: 9 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 1 \t Pred: 6 \t Eq: False\n",
      "[cat3] True: 5 \t Pred: 0 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 4 \t Pred: 4 \t Eq: True\n",
      "[cat3] True: 6 \t Pred: 9 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat3] True: 8 \t Pred: 9 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 4 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 0 \t Eq: False\n",
      "[cat2] True: 6 \t Pred: 6 \t Eq: True\n",
      "[cat3] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat3] True: 8 \t Pred: 8 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 3 \t Eq: False\n",
      "[cat2] True: 3 \t Pred: 1 \t Eq: False\n",
      "[cat3] True: 3 \t Pred: 5 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat3] True: 2 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat3] True: 5 \t Pred: 5 \t Eq: True\n",
      "[cat1] True: 1 \t Pred: 0 \t Eq: False\n",
      "[cat2] True: 0 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 9 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat2] True: 4 \t Pred: 4 \t Eq: True\n",
      "[cat3] True: 6 \t Pred: 6 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 3 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 3 \t Eq: False\n",
      "[cat2] True: 5 \t Pred: 1 \t Eq: False\n",
      "[cat3] True: 7 \t Pred: 3 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 5 \t Pred: 0 \t Eq: False\n",
      "[cat3] True: 7 \t Pred: 9 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 4 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 0 \t Eq: False\n",
      "[cat2] True: 6 \t Pred: 6 \t Eq: True\n",
      "[cat3] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat1] True: 3 \t Pred: 0 \t Eq: False\n",
      "[cat2] True: 6 \t Pred: 6 \t Eq: True\n",
      "[cat3] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 4 \t Pred: 0 \t Eq: False\n",
      "[cat3] True: 6 \t Pred: 8 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 0 \t Eq: False\n",
      "[cat2] True: 6 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 0 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat3] True: 2 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 6 \t Pred: 1 \t Eq: False\n",
      "[cat3] True: 0 \t Pred: 5 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 3 \t Eq: False\n",
      "[cat2] True: 5 \t Pred: 6 \t Eq: False\n",
      "[cat3] True: 7 \t Pred: 0 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 4 \t Pred: 0 \t Eq: False\n",
      "[cat3] True: 6 \t Pred: 8 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat3] True: 8 \t Pred: 8 \t Eq: True\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat3] True: 9 \t Pred: 9 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat1] True: 0 \t Pred: 3 \t Eq: False\n",
      "[cat2] True: 5 \t Pred: 6 \t Eq: False\n",
      "[cat3] True: 7 \t Pred: 0 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat3] True: 8 \t Pred: 8 \t Eq: True\n",
      "[cat1] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat2] True: 6 \t Pred: 6 \t Eq: True\n",
      "[cat3] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 5 \t Pred: 0 \t Eq: False\n",
      "[cat3] True: 7 \t Pred: 8 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat2] True: 5 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 7 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat2] True: 4 \t Pred: 4 \t Eq: True\n",
      "[cat3] True: 6 \t Pred: 6 \t Eq: True\n",
      "[cat1] True: 1 \t Pred: 0 \t Eq: False\n",
      "[cat2] True: 0 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 9 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 3 \t Eq: False\n",
      "[cat2] True: 5 \t Pred: 6 \t Eq: False\n",
      "[cat3] True: 7 \t Pred: 0 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 3 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 4 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 3 \t Eq: False\n",
      "[cat2] True: 3 \t Pred: 6 \t Eq: False\n",
      "[cat3] True: 3 \t Pred: 0 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat2] True: 1 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 5 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat3] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat3] True: 5 \t Pred: 5 \t Eq: True\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 9 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 3 \t Eq: False\n",
      "[cat2] True: 3 \t Pred: 1 \t Eq: False\n",
      "[cat3] True: 3 \t Pred: 5 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat2] True: 6 \t Pred: 6 \t Eq: True\n",
      "[cat3] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 3 \t Eq: False\n",
      "[cat2] True: 2 \t Pred: 1 \t Eq: False\n",
      "[cat3] True: 2 \t Pred: 5 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat3] True: 5 \t Pred: 5 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 4 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 3 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 6 \t Pred: 1 \t Eq: False\n",
      "[cat3] True: 0 \t Pred: 5 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 0 \t Eq: False\n",
      "[cat2] True: 0 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 8 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 5 \t Pred: 0 \t Eq: False\n",
      "[cat3] True: 7 \t Pred: 9 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 4 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 6 \t Pred: 1 \t Eq: False\n",
      "[cat3] True: 0 \t Pred: 5 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat3] True: 4 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 4 \t Pred: 4 \t Eq: True\n",
      "[cat3] True: 6 \t Pred: 9 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat3] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 4 \t Pred: 0 \t Eq: False\n",
      "[cat3] True: 6 \t Pred: 0 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 5 \t Pred: 0 \t Eq: False\n",
      "[cat3] True: 7 \t Pred: 9 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat3] True: 4 \t Pred: 3 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 5 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 7 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat3] True: 2 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat3] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 4 \t Pred: 0 \t Eq: False\n",
      "[cat3] True: 6 \t Pred: 9 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat3] True: 9 \t Pred: 8 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat2] True: 5 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 7 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat3] True: 8 \t Pred: 9 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 1 \t Pred: 6 \t Eq: False\n",
      "[cat3] True: 5 \t Pred: 0 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat3] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat3] True: 8 \t Pred: 8 \t Eq: True\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat3] True: 5 \t Pred: 5 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat3] True: 2 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 3 \t Eq: False\n",
      "[cat2] True: 3 \t Pred: 6 \t Eq: False\n",
      "[cat3] True: 4 \t Pred: 0 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 6 \t Pred: 6 \t Eq: True\n",
      "[cat3] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat3] True: 2 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 5 \t Pred: 0 \t Eq: False\n",
      "[cat3] True: 7 \t Pred: 8 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 0 \t Eq: False\n",
      "[cat2] True: 0 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 8 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 9 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 4 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 5 \t Pred: 0 \t Eq: False\n",
      "[cat3] True: 7 \t Pred: 9 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 4 \t Pred: 4 \t Eq: True\n",
      "[cat3] True: 6 \t Pred: 9 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat3] True: 8 \t Pred: 8 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 4 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 4 \t Pred: 0 \t Eq: False\n",
      "[cat3] True: 6 \t Pred: 8 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat3] True: 2 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 5 \t Pred: 0 \t Eq: False\n",
      "[cat3] True: 7 \t Pred: 9 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 4 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat3] True: 5 \t Pred: 5 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat3] True: 5 \t Pred: 5 \t Eq: True\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 5 \t Pred: 0 \t Eq: False\n",
      "[cat3] True: 7 \t Pred: 9 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 4 \t Pred: 4 \t Eq: True\n",
      "[cat3] True: 6 \t Pred: 8 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 4 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 4 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat3] True: 8 \t Pred: 8 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 3 \t Eq: False\n",
      "[cat2] True: 3 \t Pred: 1 \t Eq: False\n",
      "[cat3] True: 3 \t Pred: 5 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat3] True: 2 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 0 \t Eq: False\n",
      "[cat2] True: 0 \t Pred: 6 \t Eq: False\n",
      "[cat3] True: 8 \t Pred: 0 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 6 \t Pred: 1 \t Eq: False\n",
      "[cat3] True: 0 \t Pred: 3 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat3] True: 5 \t Pred: 5 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat3] True: 2 \t Pred: 3 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 9 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 0 \t Eq: False\n",
      "[cat2] True: 6 \t Pred: 6 \t Eq: True\n",
      "[cat3] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat3] True: 2 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat3] True: 8 \t Pred: 9 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat3] True: 5 \t Pred: 5 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat3] True: 1 \t Pred: 3 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 4 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 9 \t Pred: 9 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 3 \t Eq: False\n",
      "[cat2] True: 3 \t Pred: 1 \t Eq: False\n",
      "[cat3] True: 3 \t Pred: 5 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 4 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 5 \t Pred: 0 \t Eq: False\n",
      "[cat3] True: 7 \t Pred: 9 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 6 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 0 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat3] True: 3 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 5 \t Pred: 0 \t Eq: False\n",
      "[cat3] True: 7 \t Pred: 9 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat3] True: 8 \t Pred: 8 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat3] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat1] True: 1 \t Pred: 0 \t Eq: False\n",
      "[cat2] True: 0 \t Pred: 6 \t Eq: False\n",
      "[cat3] True: 8 \t Pred: 0 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 1 \t Pred: 6 \t Eq: False\n",
      "[cat3] True: 5 \t Pred: 0 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 6 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 0 \t Pred: 8 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 4 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 3 \t Eq: False\n",
      "[cat2] True: 3 \t Pred: 1 \t Eq: False\n",
      "[cat3] True: 4 \t Pred: 5 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 9 \t Pred: 8 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 5 \t Pred: 0 \t Eq: False\n",
      "[cat3] True: 7 \t Pred: 8 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat3] True: 5 \t Pred: 5 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat3] True: 2 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 2 \t Pred: 3 \t Eq: False\n",
      "[cat3] True: 2 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat3] True: 9 \t Pred: 8 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat3] True: 4 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat3] True: 9 \t Pred: 8 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 3 \t Eq: False\n",
      "[cat2] True: 4 \t Pred: 6 \t Eq: False\n",
      "[cat3] True: 6 \t Pred: 0 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat3] True: 9 \t Pred: 0 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 0 \t Eq: False\n",
      "[cat2] True: 0 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 9 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat3] True: 5 \t Pred: 5 \t Eq: True\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 9 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 6 \t Pred: 1 \t Eq: False\n",
      "[cat3] True: 0 \t Pred: 5 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 3 \t Eq: False\n",
      "[cat2] True: 3 \t Pred: 1 \t Eq: False\n",
      "[cat3] True: 3 \t Pred: 5 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 4 \t Pred: 0 \t Eq: False\n",
      "[cat3] True: 6 \t Pred: 8 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat3] True: 2 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 3 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat3] True: 4 \t Pred: 3 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 3 \t Eq: False\n",
      "[cat2] True: 3 \t Pred: 1 \t Eq: False\n",
      "[cat3] True: 3 \t Pred: 5 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat3] True: 5 \t Pred: 5 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 3 \t Eq: False\n",
      "[cat2] True: 3 \t Pred: 1 \t Eq: False\n",
      "[cat3] True: 3 \t Pred: 5 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 4 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat2] True: 4 \t Pred: 4 \t Eq: True\n",
      "[cat3] True: 6 \t Pred: 6 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 2 \t Pred: 3 \t Eq: False\n",
      "[cat3] True: 2 \t Pred: 3 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat3] True: 2 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat3] True: 8 \t Pred: 8 \t Eq: True\n",
      "[cat1] True: 1 \t Pred: 0 \t Eq: False\n",
      "[cat2] True: 0 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 9 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 0 \t Eq: False\n",
      "[cat2] True: 6 \t Pred: 6 \t Eq: True\n",
      "[cat3] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat1] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat2] True: 5 \t Pred: 6 \t Eq: False\n",
      "[cat3] True: 7 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat3] True: 1 \t Pred: 3 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 0 \t Eq: False\n",
      "[cat2] True: 6 \t Pred: 6 \t Eq: True\n",
      "[cat3] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat3] True: 5 \t Pred: 5 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat1] True: 1 \t Pred: 0 \t Eq: False\n",
      "[cat2] True: 0 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 8 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 4 \t Pred: 0 \t Eq: False\n",
      "[cat3] True: 6 \t Pred: 8 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 4 \t Pred: 0 \t Eq: False\n",
      "[cat3] True: 6 \t Pred: 8 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 6 \t Pred: 1 \t Eq: False\n",
      "[cat3] True: 0 \t Pred: 3 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat3] True: 2 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat3] True: 9 \t Pred: 8 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 0 \t Eq: False\n",
      "[cat2] True: 0 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 8 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat3] True: 5 \t Pred: 5 \t Eq: True\n",
      "[cat1] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat2] True: 4 \t Pred: 4 \t Eq: True\n",
      "[cat3] True: 6 \t Pred: 6 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat3] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 9 \t Pred: 9 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 2 \t Pred: 3 \t Eq: False\n",
      "[cat3] True: 2 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 4 \t Pred: 0 \t Eq: False\n",
      "[cat3] True: 6 \t Pred: 8 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat3] True: 2 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 6 \t Pred: 5 \t Eq: False\n",
      "[cat3] True: 0 \t Pred: 3 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 4 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat3] True: 9 \t Pred: 8 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 2 \t Pred: 3 \t Eq: False\n",
      "[cat3] True: 2 \t Pred: 3 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 4 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 9 \t Pred: 9 \t Eq: True\n",
      "[cat1] True: 1 \t Pred: 0 \t Eq: False\n",
      "[cat2] True: 0 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 9 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 4 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 3 \t Eq: False\n",
      "[cat2] True: 3 \t Pred: 1 \t Eq: False\n",
      "[cat3] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat1] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat2] True: 5 \t Pred: 6 \t Eq: False\n",
      "[cat3] True: 7 \t Pred: 0 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat3] True: 9 \t Pred: 8 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat3] True: 8 \t Pred: 8 \t Eq: True\n",
      "[cat1] True: 1 \t Pred: 0 \t Eq: False\n",
      "[cat2] True: 0 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 9 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat3] True: 2 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 4 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat3] True: 5 \t Pred: 5 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat3] True: 2 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat3] True: 5 \t Pred: 5 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat1] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat2] True: 1 \t Pred: 3 \t Eq: False\n",
      "[cat3] True: 5 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 0 \t Eq: False\n",
      "[cat2] True: 3 \t Pred: 6 \t Eq: False\n",
      "[cat3] True: 3 \t Pred: 0 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 6 \t Pred: 0 \t Eq: False\n",
      "[cat3] True: 0 \t Pred: 8 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat2] True: 4 \t Pred: 4 \t Eq: True\n",
      "[cat3] True: 6 \t Pred: 6 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 3 \t Eq: False\n",
      "[cat2] True: 3 \t Pred: 1 \t Eq: False\n",
      "[cat3] True: 3 \t Pred: 5 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 4 \t Pred: 4 \t Eq: True\n",
      "[cat3] True: 6 \t Pred: 6 \t Eq: True\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 5 \t Pred: 0 \t Eq: False\n",
      "[cat3] True: 7 \t Pred: 8 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat3] True: 4 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat3] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat3] True: 8 \t Pred: 9 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat3] True: 5 \t Pred: 5 \t Eq: True\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 6 \t Pred: 1 \t Eq: False\n",
      "[cat3] True: 0 \t Pred: 5 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat2] True: 4 \t Pred: 4 \t Eq: True\n",
      "[cat3] True: 6 \t Pred: 6 \t Eq: True\n",
      "[cat1] True: 3 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 6 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 0 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 3 \t Pred: 0 \t Eq: False\n",
      "[cat3] True: 3 \t Pred: 8 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat3] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 6 \t Pred: 6 \t Eq: True\n",
      "[cat3] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat3] True: 8 \t Pred: 8 \t Eq: True\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 5 \t Pred: 0 \t Eq: False\n",
      "[cat3] True: 7 \t Pred: 8 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 0 \t Eq: False\n",
      "[cat2] True: 0 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 9 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 2 \t Pred: 3 \t Eq: False\n",
      "[cat3] True: 2 \t Pred: 3 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat3] True: 2 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat3] True: 5 \t Pred: 5 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 3 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 0 \t Eq: False\n",
      "[cat2] True: 0 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 9 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 4 \t Pred: 4 \t Eq: True\n",
      "[cat3] True: 6 \t Pred: 6 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat3] True: 2 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 4 \t Pred: 4 \t Eq: True\n",
      "[cat3] True: 6 \t Pred: 6 \t Eq: True\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat3] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat1] True: 3 \t Pred: 3 \t Eq: True\n",
      "[cat2] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat3] True: 5 \t Pred: 5 \t Eq: True\n",
      "[cat1] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat2] True: 4 \t Pred: 4 \t Eq: True\n",
      "[cat3] True: 6 \t Pred: 6 \t Eq: True\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 4 \t Pred: 4 \t Eq: True\n",
      "[cat3] True: 6 \t Pred: 9 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat1] True: 1 \t Pred: 0 \t Eq: False\n",
      "[cat2] True: 0 \t Pred: 6 \t Eq: False\n",
      "[cat3] True: 8 \t Pred: 0 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 2 \t Eq: False\n",
      "[cat2] True: 4 \t Pred: 3 \t Eq: False\n",
      "[cat3] True: 6 \t Pred: 3 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 8 \t Pred: 9 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 4 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat3] True: 2 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat2] True: 4 \t Pred: 4 \t Eq: True\n",
      "[cat3] True: 6 \t Pred: 6 \t Eq: True\n",
      "[cat1] True: 1 \t Pred: 1 \t Eq: True\n",
      "[cat2] True: 0 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 8 \t Pred: 9 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat2] True: 5 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 7 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 2 \t Pred: 2 \t Eq: True\n",
      "[cat2] True: 3 \t Pred: 2 \t Eq: False\n",
      "[cat3] True: 4 \t Pred: 1 \t Eq: False\n",
      "[cat1] True: 1 \t Pred: 0 \t Eq: False\n",
      "[cat2] True: 0 \t Pred: 4 \t Eq: False\n",
      "[cat3] True: 9 \t Pred: 6 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 0 \t Eq: True\n",
      "[cat2] True: 5 \t Pred: 6 \t Eq: False\n",
      "[cat3] True: 7 \t Pred: 0 \t Eq: False\n",
      "[cat1] True: 0 \t Pred: 1 \t Eq: False\n",
      "[cat2] True: 4 \t Pred: 0 \t Eq: False\n",
      "[cat3] True: 6 \t Pred: 9 \t Eq: False\n"
     ]
    }
   ],
   "source": [
    "for inputs, labels in dataloaders['test']:\n",
    "    outputs = get_cat(inputs)\n",
    "    break\n",
    "\n",
    "outputs = [outputs[0].detach().numpy(),\n",
    "           outputs[1].detach().numpy(),\n",
    "           outputs[2].detach().numpy()]\n",
    "outputs = [[o1, o2, o3] for o1, o2, o3 in zip(outputs[0], outputs[1], outputs[2])]\n",
    "labels = labels.detach().numpy() \n",
    "for output, label in zip(outputs, labels):\n",
    "    for i in range(3):\n",
    "        print(f'[cat{i+1}] True: {label[i]} \\t Pred: {output[i]} \\t Eq: {output[i] == label[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qTCgF_y3tVh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
